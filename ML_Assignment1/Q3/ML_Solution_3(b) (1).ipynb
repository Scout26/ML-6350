{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tBzycRJC13AV",
        "outputId": "1a399246-a581-471c-c992-0e0b9dd9a7b4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Solution for Part 3(B) BELOW...\n",
            "----- Training Decision Tree with Depth 1 and Method of Split as entropy -----\n",
            "Training Missclassified Points(indicating underfitting):  596\n",
            "Training Error:  0.1192\n",
            "Testing Missclassified Points(indicating overfitting):  624\n",
            "Testing Error 0.1248\n",
            "**************************************************\n",
            "----- Training Decision Tree with Depth 1 and Method of Split as gini -----\n",
            "Training Missclassified Points(indicating underfitting):  544\n",
            "Training Error:  0.1088\n",
            "Testing Missclassified Points(indicating overfitting):  583\n",
            "Testing Error 0.1166\n",
            "**************************************************\n",
            "----- Training Decision Tree with Depth 1 and Method of Split as MajorityError -----\n",
            "Training Missclassified Points(indicating underfitting):  596\n",
            "Training Error:  0.1192\n",
            "Testing Missclassified Points(indicating overfitting):  624\n",
            "Testing Error 0.1248\n",
            "**************************************************\n",
            "----- Training Decision Tree with Depth 2 and Method of Split as entropy -----\n",
            "Training Missclassified Points(indicating underfitting):  530\n",
            "Training Error:  0.106\n",
            "Testing Missclassified Points(indicating overfitting):  557\n",
            "Testing Error 0.1114\n",
            "**************************************************\n",
            "----- Training Decision Tree with Depth 2 and Method of Split as gini -----\n",
            "Training Missclassified Points(indicating underfitting):  526\n",
            "Training Error:  0.1052\n",
            "Testing Missclassified Points(indicating overfitting):  552\n",
            "Testing Error 0.1104\n",
            "**************************************************\n",
            "----- Training Decision Tree with Depth 2 and Method of Split as MajorityError -----\n",
            "Training Missclassified Points(indicating underfitting):  596\n",
            "Training Error:  0.1192\n",
            "Testing Missclassified Points(indicating overfitting):  624\n",
            "Testing Error 0.1248\n",
            "**************************************************\n",
            "----- Training Decision Tree with Depth 3 and Method of Split as entropy -----\n",
            "Training Missclassified Points(indicating underfitting):  511\n",
            "Training Error:  0.1022\n",
            "Testing Missclassified Points(indicating overfitting):  543\n",
            "Testing Error 0.1086\n",
            "**************************************************\n",
            "----- Training Decision Tree with Depth 3 and Method of Split as gini -----\n",
            "Training Missclassified Points(indicating underfitting):  505\n",
            "Training Error:  0.101\n",
            "Testing Missclassified Points(indicating overfitting):  537\n",
            "Testing Error 0.1074\n",
            "**************************************************\n",
            "----- Training Decision Tree with Depth 3 and Method of Split as MajorityError -----\n",
            "Training Missclassified Points(indicating underfitting):  596\n",
            "Training Error:  0.1192\n",
            "Testing Missclassified Points(indicating overfitting):  627\n",
            "Testing Error 0.1254\n",
            "**************************************************\n",
            "----- Training Decision Tree with Depth 4 and Method of Split as entropy -----\n",
            "Training Missclassified Points(indicating underfitting):  438\n",
            "Training Error:  0.0876\n",
            "Testing Missclassified Points(indicating overfitting):  600\n",
            "Testing Error 0.12\n",
            "**************************************************\n",
            "----- Training Decision Tree with Depth 4 and Method of Split as gini -----\n",
            "Training Missclassified Points(indicating underfitting):  441\n",
            "Training Error:  0.0882\n",
            "Testing Missclassified Points(indicating overfitting):  578\n",
            "Testing Error 0.1156\n",
            "**************************************************\n",
            "----- Training Decision Tree with Depth 4 and Method of Split as MajorityError -----\n",
            "Training Missclassified Points(indicating underfitting):  585\n",
            "Training Error:  0.117\n",
            "Testing Missclassified Points(indicating overfitting):  621\n",
            "Testing Error 0.1242\n",
            "**************************************************\n",
            "----- Training Decision Tree with Depth 5 and Method of Split as entropy -----\n",
            "Training Missclassified Points(indicating underfitting):  349\n",
            "Training Error:  0.0698\n",
            "Testing Missclassified Points(indicating overfitting):  635\n",
            "Testing Error 0.127\n",
            "**************************************************\n",
            "----- Training Decision Tree with Depth 5 and Method of Split as gini -----\n",
            "Training Missclassified Points(indicating underfitting):  360\n",
            "Training Error:  0.072\n",
            "Testing Missclassified Points(indicating overfitting):  625\n",
            "Testing Error 0.125\n",
            "**************************************************\n",
            "----- Training Decision Tree with Depth 5 and Method of Split as MajorityError -----\n",
            "Training Missclassified Points(indicating underfitting):  570\n",
            "Training Error:  0.114\n",
            "Testing Missclassified Points(indicating overfitting):  631\n",
            "Testing Error 0.1262\n",
            "**************************************************\n",
            "----- Training Decision Tree with Depth 6 and Method of Split as entropy -----\n",
            "Training Missclassified Points(indicating underfitting):  279\n",
            "Training Error:  0.0558\n",
            "Testing Missclassified Points(indicating overfitting):  655\n",
            "Testing Error 0.131\n",
            "**************************************************\n",
            "----- Training Decision Tree with Depth 6 and Method of Split as gini -----\n",
            "Training Missclassified Points(indicating underfitting):  282\n",
            "Training Error:  0.0564\n",
            "Testing Missclassified Points(indicating overfitting):  645\n",
            "Testing Error 0.129\n",
            "**************************************************\n",
            "----- Training Decision Tree with Depth 6 and Method of Split as MajorityError -----\n",
            "Training Missclassified Points(indicating underfitting):  523\n",
            "Training Error:  0.1046\n",
            "Testing Missclassified Points(indicating overfitting):  640\n",
            "Testing Error 0.128\n",
            "**************************************************\n",
            "----- Training Decision Tree with Depth 7 and Method of Split as entropy -----\n",
            "Training Missclassified Points(indicating underfitting):  222\n",
            "Training Error:  0.0444\n",
            "Testing Missclassified Points(indicating overfitting):  698\n",
            "Testing Error 0.1396\n",
            "**************************************************\n",
            "----- Training Decision Tree with Depth 7 and Method of Split as gini -----\n",
            "Training Missclassified Points(indicating underfitting):  222\n",
            "Training Error:  0.0444\n",
            "Testing Missclassified Points(indicating overfitting):  706\n",
            "Testing Error 0.1412\n",
            "**************************************************\n",
            "----- Training Decision Tree with Depth 7 and Method of Split as MajorityError -----\n",
            "Training Missclassified Points(indicating underfitting):  489\n",
            "Training Error:  0.0978\n",
            "Testing Missclassified Points(indicating overfitting):  615\n",
            "Testing Error 0.123\n",
            "**************************************************\n",
            "----- Training Decision Tree with Depth 8 and Method of Split as entropy -----\n",
            "Training Missclassified Points(indicating underfitting):  182\n",
            "Training Error:  0.0364\n",
            "Testing Missclassified Points(indicating overfitting):  718\n",
            "Testing Error 0.1436\n",
            "**************************************************\n",
            "----- Training Decision Tree with Depth 8 and Method of Split as gini -----\n",
            "Training Missclassified Points(indicating underfitting):  175\n",
            "Training Error:  0.035\n",
            "Testing Missclassified Points(indicating overfitting):  716\n",
            "Testing Error 0.1432\n",
            "**************************************************\n",
            "----- Training Decision Tree with Depth 8 and Method of Split as MajorityError -----\n",
            "Training Missclassified Points(indicating underfitting):  437\n",
            "Training Error:  0.0874\n",
            "Testing Missclassified Points(indicating overfitting):  652\n",
            "Testing Error 0.1304\n",
            "**************************************************\n",
            "----- Training Decision Tree with Depth 9 and Method of Split as entropy -----\n",
            "Training Missclassified Points(indicating underfitting):  156\n",
            "Training Error:  0.0312\n",
            "Testing Missclassified Points(indicating overfitting):  738\n",
            "Testing Error 0.1476\n",
            "**************************************************\n",
            "----- Training Decision Tree with Depth 9 and Method of Split as gini -----\n",
            "Training Missclassified Points(indicating underfitting):  148\n",
            "Training Error:  0.0296\n",
            "Testing Missclassified Points(indicating overfitting):  742\n",
            "Testing Error 0.1484\n",
            "**************************************************\n",
            "----- Training Decision Tree with Depth 9 and Method of Split as MajorityError -----\n",
            "Training Missclassified Points(indicating underfitting):  397\n",
            "Training Error:  0.0794\n",
            "Testing Missclassified Points(indicating overfitting):  697\n",
            "Testing Error 0.1394\n",
            "**************************************************\n",
            "----- Training Decision Tree with Depth 10 and Method of Split as entropy -----\n",
            "Training Missclassified Points(indicating underfitting):  122\n",
            "Training Error:  0.0244\n",
            "Testing Missclassified Points(indicating overfitting):  766\n",
            "Testing Error 0.1532\n",
            "**************************************************\n",
            "----- Training Decision Tree with Depth 10 and Method of Split as gini -----\n",
            "Training Missclassified Points(indicating underfitting):  117\n",
            "Training Error:  0.0234\n",
            "Testing Missclassified Points(indicating overfitting):  762\n",
            "Testing Error 0.1524\n",
            "**************************************************\n",
            "----- Training Decision Tree with Depth 10 and Method of Split as MajorityError -----\n",
            "Training Missclassified Points(indicating underfitting):  359\n",
            "Training Error:  0.0718\n",
            "Testing Missclassified Points(indicating overfitting):  713\n",
            "Testing Error 0.1426\n",
            "**************************************************\n",
            "----- Training Decision Tree with Depth 11 and Method of Split as entropy -----\n",
            "Training Missclassified Points(indicating underfitting):  105\n",
            "Training Error:  0.021\n",
            "Testing Missclassified Points(indicating overfitting):  771\n",
            "Testing Error 0.1542\n",
            "**************************************************\n",
            "----- Training Decision Tree with Depth 11 and Method of Split as gini -----\n",
            "Training Missclassified Points(indicating underfitting):  102\n",
            "Training Error:  0.0204\n",
            "Testing Missclassified Points(indicating overfitting):  773\n",
            "Testing Error 0.1546\n",
            "**************************************************\n",
            "----- Training Decision Tree with Depth 11 and Method of Split as MajorityError -----\n",
            "Training Missclassified Points(indicating underfitting):  320\n",
            "Training Error:  0.064\n",
            "Testing Missclassified Points(indicating overfitting):  720\n",
            "Testing Error 0.144\n",
            "**************************************************\n",
            "----- Training Decision Tree with Depth 12 and Method of Split as entropy -----\n",
            "Training Missclassified Points(indicating underfitting):  93\n",
            "Training Error:  0.0186\n",
            "Testing Missclassified Points(indicating overfitting):  772\n",
            "Testing Error 0.1544\n",
            "**************************************************\n",
            "----- Training Decision Tree with Depth 12 and Method of Split as gini -----\n",
            "Training Missclassified Points(indicating underfitting):  94\n",
            "Training Error:  0.0188\n",
            "Testing Missclassified Points(indicating overfitting):  770\n",
            "Testing Error 0.154\n",
            "**************************************************\n",
            "----- Training Decision Tree with Depth 12 and Method of Split as MajorityError -----\n",
            "Training Missclassified Points(indicating underfitting):  259\n",
            "Training Error:  0.0518\n",
            "Testing Missclassified Points(indicating overfitting):  747\n",
            "Testing Error 0.1494\n",
            "**************************************************\n",
            "----- Training Decision Tree with Depth 13 and Method of Split as entropy -----\n",
            "Training Missclassified Points(indicating underfitting):  90\n",
            "Training Error:  0.018\n",
            "Testing Missclassified Points(indicating overfitting):  780\n",
            "Testing Error 0.156\n",
            "**************************************************\n",
            "----- Training Decision Tree with Depth 13 and Method of Split as gini -----\n",
            "Training Missclassified Points(indicating underfitting):  90\n",
            "Training Error:  0.018\n",
            "Testing Missclassified Points(indicating overfitting):  777\n",
            "Testing Error 0.1554\n",
            "**************************************************\n",
            "----- Training Decision Tree with Depth 13 and Method of Split as MajorityError -----\n",
            "Training Missclassified Points(indicating underfitting):  221\n",
            "Training Error:  0.0442\n",
            "Testing Missclassified Points(indicating overfitting):  757\n",
            "Testing Error 0.1514\n",
            "**************************************************\n",
            "----- Training Decision Tree with Depth 14 and Method of Split as entropy -----\n",
            "Training Missclassified Points(indicating underfitting):  90\n",
            "Training Error:  0.018\n",
            "Testing Missclassified Points(indicating overfitting):  777\n",
            "Testing Error 0.1554\n",
            "**************************************************\n",
            "----- Training Decision Tree with Depth 14 and Method of Split as gini -----\n",
            "Training Missclassified Points(indicating underfitting):  90\n",
            "Training Error:  0.018\n",
            "Testing Missclassified Points(indicating overfitting):  774\n",
            "Testing Error 0.1548\n",
            "**************************************************\n",
            "----- Training Decision Tree with Depth 14 and Method of Split as MajorityError -----\n",
            "Training Missclassified Points(indicating underfitting):  163\n",
            "Training Error:  0.0326\n",
            "Testing Missclassified Points(indicating overfitting):  793\n",
            "Testing Error 0.1586\n",
            "**************************************************\n",
            "----- Training Decision Tree with Depth 15 and Method of Split as entropy -----\n",
            "Training Missclassified Points(indicating underfitting):  90\n",
            "Training Error:  0.018\n",
            "Testing Missclassified Points(indicating overfitting):  776\n",
            "Testing Error 0.1552\n",
            "**************************************************\n",
            "----- Training Decision Tree with Depth 15 and Method of Split as gini -----\n",
            "Training Missclassified Points(indicating underfitting):  90\n",
            "Training Error:  0.018\n",
            "Testing Missclassified Points(indicating overfitting):  772\n",
            "Testing Error 0.1544\n",
            "**************************************************\n",
            "----- Training Decision Tree with Depth 15 and Method of Split as MajorityError -----\n",
            "Training Missclassified Points(indicating underfitting):  117\n",
            "Training Error:  0.0234\n",
            "Testing Missclassified Points(indicating overfitting):  789\n",
            "Testing Error 0.1578\n",
            "**************************************************\n",
            "----- Training Decision Tree with Depth 16 and Method of Split as entropy -----\n",
            "Training Missclassified Points(indicating underfitting):  90\n",
            "Training Error:  0.018\n",
            "Testing Missclassified Points(indicating overfitting):  776\n",
            "Testing Error 0.1552\n",
            "**************************************************\n",
            "----- Training Decision Tree with Depth 16 and Method of Split as gini -----\n",
            "Training Missclassified Points(indicating underfitting):  90\n",
            "Training Error:  0.018\n",
            "Testing Missclassified Points(indicating overfitting):  772\n",
            "Testing Error 0.1544\n",
            "**************************************************\n",
            "----- Training Decision Tree with Depth 16 and Method of Split as MajorityError -----\n",
            "Training Missclassified Points(indicating underfitting):  90\n",
            "Training Error:  0.018\n",
            "Testing Missclassified Points(indicating overfitting):  796\n",
            "Testing Error 0.1592\n",
            "**************************************************\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import math\n",
        "import pandas as pd\n",
        "\n",
        "training_data = pd.read_csv(\"train.csv\",header=None)\n",
        "\n",
        "print(\"Solution for Part 3(B) BELOW...\")\n",
        "\n",
        "columns = [\"age\",\"job\",\"marital\",\"education\",\"default\",\"balance\",\"housing\",\"loan\",\"contact\",\"day\",\"month\",\"duration\",\"campaign\",\"pdays\",\"previous\",\"poutcome\",\"y\"]\n",
        "\n",
        "training_data.columns = columns\n",
        "train_data_backup = training_data.copy()\n",
        "\n",
        "#Columns present inside data-desc file\n",
        "\n",
        "maximum_fea = dict()\n",
        "for i in columns:\n",
        "    temp_dict = dict(training_data[i].value_counts())\n",
        "    if \"unknown\" in temp_dict.keys():\n",
        "        del temp_dict['unknown']\n",
        "        maximum_fea[i] = max(temp_dict,key=temp_dict.get)\n",
        "        training_data[i] = training_data[i].replace(['unknown'],maximum_fea[i])\n",
        "\n",
        "def number_to_bin_age(x):\n",
        "    if x>np.median(training_data['age']):\n",
        "        return \"yes\"\n",
        "    else:\n",
        "        return \"no\"\n",
        "def number_to_bin_balance(x):\n",
        "    if x>np.median(training_data['balance']):\n",
        "        return \"yes\"\n",
        "    else:\n",
        "        return \"no\"\n",
        "def number_to_bin_day(x):\n",
        "    if x>np.median(training_data['day']):\n",
        "        return \"yes\"\n",
        "    else:\n",
        "        return \"no\"\n",
        "def number_to_bin_duration(x):\n",
        "    if x>np.median(training_data['duration']):\n",
        "        return \"yes\"\n",
        "    else:\n",
        "        return \"no\"\n",
        "def number_to_bin_campaign(x):\n",
        "    if x>np.median(training_data['campaign']):\n",
        "        return \"yes\"\n",
        "    else:\n",
        "        return \"no\"\n",
        "def number_to_bin_pdays(x):\n",
        "    if x>np.median(training_data['pdays']):\n",
        "        return \"yes\"\n",
        "    else:\n",
        "        return \"no\"\n",
        "def number_to_bin_previous(x):\n",
        "    if x>np.median(training_data['previous']):\n",
        "        return \"yes\"\n",
        "    else:\n",
        "        return \"no\"\n",
        "\n",
        "training_data['age'] = training_data['age'].map(number_to_bin_age)\n",
        "training_data['balance'] = training_data['balance'].map(number_to_bin_balance)\n",
        "training_data['day'] = training_data['day'].map(number_to_bin_day)\n",
        "training_data['duration'] = training_data['duration'].map(number_to_bin_duration)\n",
        "training_data['campaign'] = training_data['campaign'].map(number_to_bin_campaign)\n",
        "training_data['pdays'] = training_data['pdays'].map(number_to_bin_pdays)\n",
        "training_data['previous'] = training_data['previous'].map(number_to_bin_previous)\n",
        "\n",
        "test_data = pd.read_csv(\"test.csv\",header=None)\n",
        "test_data.columns = columns\n",
        "\n",
        "def number_to_bin_age(x):\n",
        "    if x>np.median(train_data_backup['age']):\n",
        "        return \"yes\"\n",
        "    else:\n",
        "        return \"no\"\n",
        "def number_to_bin_balance(x):\n",
        "    if x>np.median(train_data_backup['balance']):\n",
        "        return \"yes\"\n",
        "    else:\n",
        "        return \"no\"\n",
        "def number_to_bin_day(x):\n",
        "    if x>np.median(train_data_backup['day']):\n",
        "        return \"yes\"\n",
        "    else:\n",
        "        return \"no\"\n",
        "def number_to_bin_duration(x):\n",
        "    if x>np.median(train_data_backup['duration']):\n",
        "        return \"yes\"\n",
        "    else:\n",
        "        return \"no\"\n",
        "def number_to_bin_campaign(x):\n",
        "    if x>np.median(train_data_backup['campaign']):\n",
        "        return \"yes\"\n",
        "    else:\n",
        "        return \"no\"\n",
        "def number_to_bin_pdays(x):\n",
        "    if x>np.median(train_data_backup['pdays']):\n",
        "        return \"yes\"\n",
        "    else:\n",
        "        return \"no\"\n",
        "def number_to_bin_previous(x):\n",
        "    if x>np.median(train_data_backup['previous']):\n",
        "        return \"yes\"\n",
        "    else:\n",
        "        return \"no\"\n",
        "\n",
        "test_data['age'] = test_data['age'].map(number_to_bin_age)\n",
        "test_data['balance'] = test_data['balance'].map(number_to_bin_balance)\n",
        "test_data['day'] = test_data['day'].map(number_to_bin_day)\n",
        "test_data['duration'] = test_data['duration'].map(number_to_bin_duration)\n",
        "test_data['campaign'] = test_data['campaign'].map(number_to_bin_campaign)\n",
        "test_data['pdays'] = test_data['pdays'].map(number_to_bin_pdays)\n",
        "test_data['previous'] = test_data['previous'].map(number_to_bin_previous)\n",
        "\n",
        "maximum_fea = dict()\n",
        "for i in columns:\n",
        "    temp_dict = dict(train_data_backup[i].value_counts())\n",
        "    if \"unknown\" in temp_dict.keys():\n",
        "        del temp_dict['unknown']\n",
        "        maximum_fea[i] = max(temp_dict,key=temp_dict.get)\n",
        "        test_data[i] = test_data[i].replace(['unknown'],maximum_fea[i])\n",
        "\n",
        "#Training Data alon X and Y\n",
        "X_train = training_data.iloc[:,:-1]\n",
        "Y_train = training_data.iloc[:,-1]\n",
        "\n",
        "#Testing Data alon X and Y\n",
        "X_test = test_data.iloc[:,:-1]\n",
        "Y_test = test_data.iloc[:,-1]\n",
        "\n",
        "#Function for ID3:\n",
        "class ID3:\n",
        "    def __init__(self):\n",
        "        self.data = None\n",
        "        self.features = None\n",
        "        self.labels = None\n",
        "    def total_entropy(self,data,labels):\n",
        "        label_data = data['y'].value_counts()\n",
        "        e = 0\n",
        "        for i in label_data:\n",
        "            e -= (i/sum(label_data)) * math.log2(i/sum(label_data))\n",
        "        return e\n",
        "    def fea_cat_entropy(self,data, feature,labels):\n",
        "        categories_features = data[feature].value_counts().keys()\n",
        "        e = 0\n",
        "        for cat in categories_features:\n",
        "            label_fea_data = data[data[feature]==cat]['y'].value_counts()\n",
        "            pd.DataFrame(data)[feature].value_counts()\n",
        "            s = 0\n",
        "            for i in label_fea_data:\n",
        "                s -= (i/sum(label_fea_data)) * math.log2(i/sum(label_fea_data))\n",
        "            e += (sum(label_fea_data)/len(data)) * (s)\n",
        "        return e\n",
        "    def total_gini(self,data,labels):\n",
        "        label_data = data['y'].value_counts()\n",
        "        e = 1\n",
        "        for i in label_data:\n",
        "            e -= (i/sum(label_data))**2\n",
        "        return e\n",
        "    def fea_cat_gini(self,data, feature,labels):\n",
        "        categories_features = data[feature].value_counts().keys()\n",
        "        e = 0\n",
        "        for cat in categories_features:\n",
        "            label_fea_data = data[data[feature]==cat]['y'].value_counts()\n",
        "            pd.DataFrame(data)[feature].value_counts()\n",
        "            s = 1\n",
        "            for i in label_fea_data:\n",
        "                s -= (i/sum(label_fea_data))**2\n",
        "            e += (sum(label_fea_data)/len(data)) * (s)\n",
        "        return e\n",
        "\n",
        "    def total_me(self,data,labels):\n",
        "        label_data = data['y'].value_counts()\n",
        "        e = (min(label_data)/sum(label_data))\n",
        "        return e\n",
        "    def fea_cat_me(self,data, feature,labels):\n",
        "        categories_features = data[feature].value_counts().keys()\n",
        "        e = 0\n",
        "        for cat in categories_features:\n",
        "            label_fea_data = data[data[feature]==cat]['y'].value_counts()\n",
        "            pd.DataFrame(data)[feature].value_counts()\n",
        "            if len(label_fea_data)!=4:\n",
        "                   e+=0\n",
        "            else:\n",
        "                e += (min(label_fea_data)/len(data))\n",
        "        return e\n",
        "    def IG(self,data,features,labels,split_method):\n",
        "        if split_method==\"entropy\":\n",
        "          return self.total_entropy(data,labels) - self.fea_cat_entropy(data,features,labels)\n",
        "        elif split_method==\"gini\":\n",
        "          return self.total_gini(data,labels) - self.fea_cat_gini(data,features,labels)\n",
        "        elif split_method==\"MajorityError\":\n",
        "          return self.total_me(data,labels) - self.fea_cat_me(data,features,labels)\n",
        "    def create_root_node(self,data,features,split_method,labels):\n",
        "        total_fea_ig = dict()\n",
        "        for i in features:\n",
        "            total_fea_ig[i] = self.IG(data,i,labels,split_method)\n",
        "        best_feature = max(total_fea_ig, key=total_fea_ig.get)\n",
        "        return best_feature\n",
        "    def find_bestsplits(self,data,data_copy,max_depth,depth,best_features):\n",
        "        features_groups = dict()\n",
        "        temp_x = []\n",
        "        if depth<max_depth:\n",
        "          for x in data[best_features].value_counts().keys():\n",
        "              for i in range(len(data)):\n",
        "                  if data.iloc[i][best_features] == x:\n",
        "                      if features_groups.get(x,0)==0:\n",
        "                          features_groups[x] = [(dict(data.iloc[i][:-1]),data.iloc[i]['y'])]\n",
        "                      else:\n",
        "                          features_groups[x].append((dict(data.iloc[i][:-1]),data.iloc[i]['y']))\n",
        "              temp_x.append(x)\n",
        "          c_ = list(data_copy[best_features].value_counts().keys())\n",
        "          for i in c_:\n",
        "              if i not in temp_x:\n",
        "                label_counts = dict(data['y'].value_counts())\n",
        "                features_groups[i] =  [({}, max(label_counts,key=label_counts.get))]\n",
        "          return features_groups,depth+1\n",
        "        else:\n",
        "          c_ = list(data_copy[best_features].value_counts().keys())\n",
        "          temp_x = list(data[best_features].value_counts().keys())\n",
        "          for i in c_:\n",
        "                if i in temp_x:\n",
        "                  label_counts = dict(data[data[best_features]==i]['y'].value_counts())\n",
        "                  features_groups[i] =  [({}, max(label_counts,key=label_counts.get))]\n",
        "                else:\n",
        "                  label_counts = dict(data['y'].value_counts())\n",
        "                  features_groups[i] =  [({}, max(label_counts,key=label_counts.get))]\n",
        "          return features_groups,depth\n",
        "#Function  ID3\n",
        "    def ID3_Algo(self,data,data_copy,features,labels,max_depth,depth,split_method,backup_features=[]):\n",
        "        if len(data['y'].value_counts()) == 1:\n",
        "            return data['y'].value_counts().keys()\n",
        "        if len(features)==0:\n",
        "            label_counts = dict(data['y'].value_counts())\n",
        "            return max(label_counts,key=label_counts.get)\n",
        "        root_node = self.create_root_node(data,features,split_method,labels)\n",
        "        categories = data[root_node].value_counts().keys()\n",
        "        if(root_node in features):\n",
        "            features.remove(root_node)\n",
        "            backup_features.append(root_node)\n",
        "        feature_groups,depth = self.find_bestsplits(data, data_copy,max_depth,depth,root_node)\n",
        "        subtree_dict = {}\n",
        "        final_tree = tuple()\n",
        "        for i ,j in feature_groups.items():\n",
        "            data = pd.DataFrame.from_dict({k: dict(v) for k,v in pd.DataFrame(j)[0].items()}, orient='index')\n",
        "            data['y'] = pd.DataFrame(j)[1]\n",
        "            subtree_dict[i] = self.ID3_Algo(data, data_copy, features,labels,max_depth,depth,split_method,backup_features)\n",
        "            final_tree = (root_node,subtree_dict)\n",
        "        features.append(backup_features[-1])\n",
        "        backup_features.remove(backup_features[-1])\n",
        "        return final_tree\n",
        "def classify(tree, query):\n",
        "      if tree in labels:\n",
        "          return tree\n",
        "      key = query.get(tree[0])\n",
        "      if key not in tree[1]:\n",
        "          key = None\n",
        "      class_ = classify(tree[1][key], query)\n",
        "      return class_\n",
        "\n",
        "best_split = [\"entropy\",\"gini\",\"MajorityError\"]\n",
        "train_error = dict()\n",
        "test_error = dict()\n",
        "\n",
        "for k in range(16):\n",
        "    for j in best_split:\n",
        "        print(\"-\"*5+\" Training Decision Tree with Depth \"+str(k+1)+\" and Method of Split as \"+j+\" \"+\"-\"*5)\n",
        "        algo = ID3()\n",
        "        labels = ['yes','no']\n",
        "        answer = dict()\n",
        "        max_depth = k+1\n",
        "        split_method = j\n",
        "        s = algo.ID3_Algo(training_data,training_data,columns[:-1],labels,max_depth,1,split_method,[])\n",
        "        c=0\n",
        "        for i in range(X_train.shape[0]):\n",
        "          sample = dict(X_train.iloc[i])\n",
        "          if classify(s,sample)==Y_train[i]:\n",
        "            c+=1\n",
        "        print(\"Training Missclassified Points(indicating underfitting): \",(X_train.shape[0]-c))\n",
        "        print(\"Training Error: \",(X_train.shape[0]-c)/X_train.shape[0])\n",
        "        if train_error.get(max_depth):\n",
        "            train_error[max_depth].append((j,(X_train.shape[0]-c)/X_train.shape[0]))\n",
        "        else:\n",
        "            train_error[max_depth] = [(j,(X_train.shape[0]-c)/X_train.shape[0])]\n",
        "        c=0\n",
        "        for i in range(X_test.shape[0]):\n",
        "          sample = dict(X_test.iloc[i])\n",
        "          if classify(s,sample)==Y_test[i]:\n",
        "            c+=1\n",
        "        print(\"Testing Missclassified Points(indicating overfitting): \",(X_test.shape[0]-c))\n",
        "        print(\"Testing Error\",(X_test.shape[0]-c)/X_test.shape[0])\n",
        "\n",
        "        if test_error.get(max_depth):\n",
        "            test_error[max_depth] .append((j,(X_test.shape[0]-c)/X_test.shape[0]))\n",
        "        else:\n",
        "            test_error[max_depth] = [(j,(X_test.shape[0]-c)/X_test.shape[0])]\n",
        "        print(\"*\"*50)"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Pj84m6Ut18PR"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}