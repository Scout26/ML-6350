{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "01Xxin-YWL8z",
        "outputId": "12004086-fa3c-46ca-edeb-ede8eacd6300"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Solution for Part 3(a) BELOW\n",
            "----- Training Decision Tree with Depth  = 1 and Method of Split as entropy -----\n",
            "Training Missclassified Points(indicating underfitting):  596\n",
            "Training Error 0.1192\n",
            "Testing Missclassified Points(indicating overfitting):  624\n",
            "Testing Error:  0.1248\n",
            "**************************************************\n",
            "----- Training Decision Tree with Depth  = 1 and Method of Split as gini -----\n",
            "Training Missclassified Points(indicating underfitting):  544\n",
            "Training Error 0.1088\n",
            "Testing Missclassified Points(indicating overfitting):  583\n",
            "Testing Error:  0.1166\n",
            "**************************************************\n",
            "----- Training Decision Tree with Depth  = 1 and Method of Split as MajorityError -----\n",
            "Training Missclassified Points(indicating underfitting):  596\n",
            "Training Error 0.1192\n",
            "Testing Missclassified Points(indicating overfitting):  624\n",
            "Testing Error:  0.1248\n",
            "**************************************************\n",
            "----- Training Decision Tree with Depth  = 2 and Method of Split as entropy -----\n",
            "Training Missclassified Points(indicating underfitting):  530\n",
            "Training Error 0.106\n",
            "Testing Missclassified Points(indicating overfitting):  557\n",
            "Testing Error:  0.1114\n",
            "**************************************************\n",
            "----- Training Decision Tree with Depth  = 2 and Method of Split as gini -----\n",
            "Training Missclassified Points(indicating underfitting):  521\n",
            "Training Error 0.1042\n",
            "Testing Missclassified Points(indicating overfitting):  544\n",
            "Testing Error:  0.1088\n",
            "**************************************************\n",
            "----- Training Decision Tree with Depth  = 2 and Method of Split as MajorityError -----\n",
            "Training Missclassified Points(indicating underfitting):  596\n",
            "Training Error 0.1192\n",
            "Testing Missclassified Points(indicating overfitting):  624\n",
            "Testing Error:  0.1248\n",
            "**************************************************\n",
            "----- Training Decision Tree with Depth  = 3 and Method of Split as entropy -----\n",
            "Training Missclassified Points(indicating underfitting):  503\n",
            "Training Error 0.1006\n",
            "Testing Missclassified Points(indicating overfitting):  532\n",
            "Testing Error:  0.1064\n",
            "**************************************************\n",
            "----- Training Decision Tree with Depth  = 3 and Method of Split as gini -----\n",
            "Training Missclassified Points(indicating underfitting):  468\n",
            "Training Error 0.0936\n",
            "Testing Missclassified Points(indicating overfitting):  557\n",
            "Testing Error:  0.1114\n",
            "**************************************************\n",
            "----- Training Decision Tree with Depth  = 3 and Method of Split as MajorityError -----\n",
            "Training Missclassified Points(indicating underfitting):  596\n",
            "Training Error 0.1192\n",
            "Testing Missclassified Points(indicating overfitting):  627\n",
            "Testing Error:  0.1254\n",
            "**************************************************\n",
            "----- Training Decision Tree with Depth  = 4 and Method of Split as entropy -----\n",
            "Training Missclassified Points(indicating underfitting):  400\n",
            "Training Error 0.08\n",
            "Testing Missclassified Points(indicating overfitting):  573\n",
            "Testing Error:  0.1146\n",
            "**************************************************\n",
            "----- Training Decision Tree with Depth  = 4 and Method of Split as gini -----\n",
            "Training Missclassified Points(indicating underfitting):  377\n",
            "Training Error 0.0754\n",
            "Testing Missclassified Points(indicating overfitting):  589\n",
            "Testing Error:  0.1178\n",
            "**************************************************\n",
            "----- Training Decision Tree with Depth  = 4 and Method of Split as MajorityError -----\n",
            "Training Missclassified Points(indicating underfitting):  580\n",
            "Training Error 0.116\n",
            "Testing Missclassified Points(indicating overfitting):  633\n",
            "Testing Error:  0.1266\n",
            "**************************************************\n",
            "----- Training Decision Tree with Depth  = 5 and Method of Split as entropy -----\n",
            "Training Missclassified Points(indicating underfitting):  312\n",
            "Training Error 0.0624\n",
            "Testing Missclassified Points(indicating overfitting):  622\n",
            "Testing Error:  0.1244\n",
            "**************************************************\n",
            "----- Training Decision Tree with Depth  = 5 and Method of Split as gini -----\n",
            "Training Missclassified Points(indicating underfitting):  302\n",
            "Training Error 0.0604\n",
            "Testing Missclassified Points(indicating overfitting):  640\n",
            "Testing Error:  0.128\n",
            "**************************************************\n",
            "----- Training Decision Tree with Depth  = 5 and Method of Split as MajorityError -----\n",
            "Training Missclassified Points(indicating underfitting):  556\n",
            "Training Error 0.1112\n",
            "Testing Missclassified Points(indicating overfitting):  629\n",
            "Testing Error:  0.1258\n",
            "**************************************************\n",
            "----- Training Decision Tree with Depth  = 6 and Method of Split as entropy -----\n",
            "Training Missclassified Points(indicating underfitting):  239\n",
            "Training Error 0.0478\n",
            "Testing Missclassified Points(indicating overfitting):  661\n",
            "Testing Error:  0.1322\n",
            "**************************************************\n",
            "----- Training Decision Tree with Depth  = 6 and Method of Split as gini -----\n",
            "Training Missclassified Points(indicating underfitting):  242\n",
            "Training Error 0.0484\n",
            "Testing Missclassified Points(indicating overfitting):  692\n",
            "Testing Error:  0.1384\n",
            "**************************************************\n",
            "----- Training Decision Tree with Depth  = 6 and Method of Split as MajorityError -----\n",
            "Training Missclassified Points(indicating underfitting):  525\n",
            "Training Error 0.105\n",
            "Testing Missclassified Points(indicating overfitting):  621\n",
            "Testing Error:  0.1242\n",
            "**************************************************\n",
            "----- Training Decision Tree with Depth  = 7 and Method of Split as entropy -----\n",
            "Training Missclassified Points(indicating underfitting):  186\n",
            "Training Error 0.0372\n",
            "Testing Missclassified Points(indicating overfitting):  697\n",
            "Testing Error:  0.1394\n",
            "**************************************************\n",
            "----- Training Decision Tree with Depth  = 7 and Method of Split as gini -----\n",
            "Training Missclassified Points(indicating underfitting):  182\n",
            "Training Error 0.0364\n",
            "Testing Missclassified Points(indicating overfitting):  743\n",
            "Testing Error:  0.1486\n",
            "**************************************************\n",
            "----- Training Decision Tree with Depth  = 7 and Method of Split as MajorityError -----\n",
            "Training Missclassified Points(indicating underfitting):  502\n",
            "Training Error 0.1004\n",
            "Testing Missclassified Points(indicating overfitting):  677\n",
            "Testing Error:  0.1354\n",
            "**************************************************\n",
            "----- Training Decision Tree with Depth  = 8 and Method of Split as entropy -----\n",
            "Training Missclassified Points(indicating underfitting):  146\n",
            "Training Error 0.0292\n",
            "Testing Missclassified Points(indicating overfitting):  717\n",
            "Testing Error:  0.1434\n",
            "**************************************************\n",
            "----- Training Decision Tree with Depth  = 8 and Method of Split as gini -----\n",
            "Training Missclassified Points(indicating underfitting):  134\n",
            "Training Error 0.0268\n",
            "Testing Missclassified Points(indicating overfitting):  749\n",
            "Testing Error:  0.1498\n",
            "**************************************************\n",
            "----- Training Decision Tree with Depth  = 8 and Method of Split as MajorityError -----\n",
            "Training Missclassified Points(indicating underfitting):  448\n",
            "Training Error 0.0896\n",
            "Testing Missclassified Points(indicating overfitting):  681\n",
            "Testing Error:  0.1362\n",
            "**************************************************\n",
            "----- Training Decision Tree with Depth  = 9 and Method of Split as entropy -----\n",
            "Training Missclassified Points(indicating underfitting):  111\n",
            "Training Error 0.0222\n",
            "Testing Missclassified Points(indicating overfitting):  733\n",
            "Testing Error:  0.1466\n",
            "**************************************************\n",
            "----- Training Decision Tree with Depth  = 9 and Method of Split as gini -----\n",
            "Training Missclassified Points(indicating underfitting):  107\n",
            "Training Error 0.0214\n",
            "Testing Missclassified Points(indicating overfitting):  781\n",
            "Testing Error:  0.1562\n",
            "**************************************************\n",
            "----- Training Decision Tree with Depth  = 9 and Method of Split as MajorityError -----\n",
            "Training Missclassified Points(indicating underfitting):  412\n",
            "Training Error 0.0824\n",
            "Testing Missclassified Points(indicating overfitting):  689\n",
            "Testing Error:  0.1378\n",
            "**************************************************\n",
            "----- Training Decision Tree with Depth  = 10 and Method of Split as entropy -----\n",
            "Training Missclassified Points(indicating underfitting):  90\n",
            "Training Error 0.018\n",
            "Testing Missclassified Points(indicating overfitting):  763\n",
            "Testing Error:  0.1526\n",
            "**************************************************\n",
            "----- Training Decision Tree with Depth  = 10 and Method of Split as gini -----\n",
            "Training Missclassified Points(indicating underfitting):  87\n",
            "Training Error 0.0174\n",
            "Testing Missclassified Points(indicating overfitting):  793\n",
            "Testing Error:  0.1586\n",
            "**************************************************\n",
            "----- Training Decision Tree with Depth  = 10 and Method of Split as MajorityError -----\n",
            "Training Missclassified Points(indicating underfitting):  347\n",
            "Training Error 0.0694\n",
            "Testing Missclassified Points(indicating overfitting):  718\n",
            "Testing Error:  0.1436\n",
            "**************************************************\n",
            "----- Training Decision Tree with Depth  = 11 and Method of Split as entropy -----\n",
            "Training Missclassified Points(indicating underfitting):  75\n",
            "Training Error 0.015\n",
            "Testing Missclassified Points(indicating overfitting):  768\n",
            "Testing Error:  0.1536\n",
            "**************************************************\n",
            "----- Training Decision Tree with Depth  = 11 and Method of Split as gini -----\n",
            "Training Missclassified Points(indicating underfitting):  72\n",
            "Training Error 0.0144\n",
            "Testing Missclassified Points(indicating overfitting):  803\n",
            "Testing Error:  0.1606\n",
            "**************************************************\n",
            "----- Training Decision Tree with Depth  = 11 and Method of Split as MajorityError -----\n",
            "Training Missclassified Points(indicating underfitting):  259\n",
            "Training Error 0.0518\n",
            "Testing Missclassified Points(indicating overfitting):  760\n",
            "Testing Error:  0.152\n",
            "**************************************************\n",
            "----- Training Decision Tree with Depth  = 12 and Method of Split as entropy -----\n",
            "Training Missclassified Points(indicating underfitting):  69\n",
            "Training Error 0.0138\n",
            "Testing Missclassified Points(indicating overfitting):  777\n",
            "Testing Error:  0.1554\n",
            "**************************************************\n",
            "----- Training Decision Tree with Depth  = 12 and Method of Split as gini -----\n",
            "Training Missclassified Points(indicating underfitting):  70\n",
            "Training Error 0.014\n",
            "Testing Missclassified Points(indicating overfitting):  802\n",
            "Testing Error:  0.1604\n",
            "**************************************************\n",
            "----- Training Decision Tree with Depth  = 12 and Method of Split as MajorityError -----\n",
            "Training Missclassified Points(indicating underfitting):  220\n",
            "Training Error 0.044\n",
            "Testing Missclassified Points(indicating overfitting):  772\n",
            "Testing Error:  0.1544\n",
            "**************************************************\n",
            "----- Training Decision Tree with Depth  = 13 and Method of Split as entropy -----\n",
            "Training Missclassified Points(indicating underfitting):  68\n",
            "Training Error 0.0136\n",
            "Testing Missclassified Points(indicating overfitting):  784\n",
            "Testing Error:  0.1568\n",
            "**************************************************\n",
            "----- Training Decision Tree with Depth  = 13 and Method of Split as gini -----\n",
            "Training Missclassified Points(indicating underfitting):  66\n",
            "Training Error 0.0132\n",
            "Testing Missclassified Points(indicating overfitting):  798\n",
            "Testing Error:  0.1596\n",
            "**************************************************\n",
            "----- Training Decision Tree with Depth  = 13 and Method of Split as MajorityError -----\n",
            "Training Missclassified Points(indicating underfitting):  157\n",
            "Training Error 0.0314\n",
            "Testing Missclassified Points(indicating overfitting):  771\n",
            "Testing Error:  0.1542\n",
            "**************************************************\n",
            "----- Training Decision Tree with Depth  = 14 and Method of Split as entropy -----\n",
            "Training Missclassified Points(indicating underfitting):  68\n",
            "Training Error 0.0136\n",
            "Testing Missclassified Points(indicating overfitting):  785\n",
            "Testing Error:  0.157\n",
            "**************************************************\n",
            "----- Training Decision Tree with Depth  = 14 and Method of Split as gini -----\n",
            "Training Missclassified Points(indicating underfitting):  66\n",
            "Training Error 0.0132\n",
            "Testing Missclassified Points(indicating overfitting):  799\n",
            "Testing Error:  0.1598\n",
            "**************************************************\n",
            "----- Training Decision Tree with Depth  = 14 and Method of Split as MajorityError -----\n",
            "Training Missclassified Points(indicating underfitting):  161\n",
            "Training Error 0.0322\n",
            "Testing Missclassified Points(indicating overfitting):  809\n",
            "Testing Error:  0.1618\n",
            "**************************************************\n",
            "----- Training Decision Tree with Depth  = 15 and Method of Split as entropy -----\n",
            "Training Missclassified Points(indicating underfitting):  66\n",
            "Training Error 0.0132\n",
            "Testing Missclassified Points(indicating overfitting):  783\n",
            "Testing Error:  0.1566\n",
            "**************************************************\n",
            "----- Training Decision Tree with Depth  = 15 and Method of Split as gini -----\n",
            "Training Missclassified Points(indicating underfitting):  66\n",
            "Training Error 0.0132\n",
            "Testing Missclassified Points(indicating overfitting):  797\n",
            "Testing Error:  0.1594\n",
            "**************************************************\n",
            "----- Training Decision Tree with Depth  = 15 and Method of Split as MajorityError -----\n",
            "Training Missclassified Points(indicating underfitting):  100\n",
            "Training Error 0.02\n",
            "Testing Missclassified Points(indicating overfitting):  796\n",
            "Testing Error:  0.1592\n",
            "**************************************************\n",
            "----- Training Decision Tree with Depth  = 16 and Method of Split as entropy -----\n",
            "Training Missclassified Points(indicating underfitting):  66\n",
            "Training Error 0.0132\n",
            "Testing Missclassified Points(indicating overfitting):  783\n",
            "Testing Error:  0.1566\n",
            "**************************************************\n",
            "----- Training Decision Tree with Depth  = 16 and Method of Split as gini -----\n",
            "Training Missclassified Points(indicating underfitting):  66\n",
            "Training Error 0.0132\n",
            "Testing Missclassified Points(indicating overfitting):  797\n",
            "Testing Error:  0.1594\n",
            "**************************************************\n",
            "----- Training Decision Tree with Depth  = 16 and Method of Split as MajorityError -----\n",
            "Training Missclassified Points(indicating underfitting):  66\n",
            "Training Error 0.0132\n",
            "Testing Missclassified Points(indicating overfitting):  802\n",
            "Testing Error:  0.1604\n",
            "**************************************************\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import math\n",
        "import pandas as pd\n",
        "#Created Variable training_data to read the content of train csv\n",
        "training_data = pd.read_csv(\"train.csv\",header=None)\n",
        "\n",
        "\n",
        "print(\"Solution for Part 3(a) BELOW\")\n",
        "\n",
        "\n",
        "training_data_backup = training_data.copy()\n",
        "\n",
        "#Columns present inside data-desc file\n",
        "columns = [\"age\",\"job\",\"marital\",\"education\",\"default\",\"balance\",\"housing\",\"loan\",\"contact\",\"day\",\"month\",\"duration\",\"campaign\",\"pdays\",\"previous\",\"poutcome\",\"y\"]\n",
        "\n",
        "\n",
        "# Mapping Columns\n",
        "training_data.columns = columns\n",
        "training_data_backup.columns = columns\n",
        "\n",
        "def num__to_binary_age(x):\n",
        "    if x>np.median(training_data['age']):\n",
        "        return \"yes\"\n",
        "    else:\n",
        "        return \"no\"\n",
        "def num__to_binary_balance(x):\n",
        "    if x>np.median(training_data['balance']):\n",
        "        return \"yes\"\n",
        "    else:\n",
        "        return \"no\"\n",
        "def num__to_binary_day(x):\n",
        "    if x>np.median(training_data['day']):\n",
        "        return \"yes\"\n",
        "    else:\n",
        "        return \"no\"\n",
        "def num__to_binary_duration(x):\n",
        "    if x>np.median(training_data['duration']):\n",
        "        return \"yes\"\n",
        "    else:\n",
        "        return \"no\"\n",
        "def num__to_binary_campaign(x):\n",
        "    if x>np.median(training_data['campaign']):\n",
        "        return \"yes\"\n",
        "    else:\n",
        "        return \"no\"\n",
        "def num__to_binary_pdays(x):\n",
        "    if x>np.median(training_data['pdays']):\n",
        "        return \"yes\"\n",
        "    else:\n",
        "        return \"no\"\n",
        "def num__to_binary_previous(x):\n",
        "    if x>np.median(training_data['previous']):\n",
        "        return \"yes\"\n",
        "    else:\n",
        "        return \"no\"\n",
        "\n",
        "\n",
        "# Mapping Above created Function\n",
        "\n",
        "\n",
        "training_data['age'] = training_data['age'].map(num__to_binary_age)\n",
        "training_data['balance'] = training_data['balance'].map(num__to_binary_balance)\n",
        "training_data['day'] = training_data['day'].map(num__to_binary_day)\n",
        "training_data['duration'] = training_data['duration'].map(num__to_binary_duration)\n",
        "training_data['campaign'] = training_data['campaign'].map(num__to_binary_campaign)\n",
        "training_data['pdays'] = training_data['pdays'].map(num__to_binary_pdays)\n",
        "training_data['previous'] = training_data['previous'].map(num__to_binary_previous)\n",
        "\n",
        "\n",
        "# Training the data along x and y direction.\n",
        "\n",
        "\n",
        "X_train = training_data.iloc[:,:-1]\n",
        "Y_train = training_data.iloc[:,-1]\n",
        "\n",
        "\n",
        "def num__to_binary_age(x):\n",
        "    if x>np.median(training_data_backup['age']):\n",
        "        return \"yes\"\n",
        "    else:\n",
        "        return \"no\"\n",
        "def num__to_binary_balance(x):\n",
        "    if x>np.median(training_data_backup['balance']):\n",
        "        return \"yes\"\n",
        "    else:\n",
        "        return \"no\"\n",
        "def num__to_binary_day(x):\n",
        "    if x>np.median(training_data_backup['day']):\n",
        "        return \"yes\"\n",
        "    else:\n",
        "        return \"no\"\n",
        "def num__to_binary_duration(x):\n",
        "    if x>np.median(training_data_backup['duration']):\n",
        "        return \"yes\"\n",
        "    else:\n",
        "        return \"no\"\n",
        "def num__to_binary_campaign(x):\n",
        "    if x>np.median(training_data_backup['campaign']):\n",
        "        return \"yes\"\n",
        "    else:\n",
        "        return \"no\"\n",
        "def num__to_binary_pdays(x):\n",
        "    if x>np.median(training_data_backup['pdays']):\n",
        "        return \"yes\"\n",
        "    else:\n",
        "        return \"no\"\n",
        "def num__to_binary_previous(x):\n",
        "    if x>np.median(training_data_backup['previous']):\n",
        "        return \"yes\"\n",
        "    else:\n",
        "        return \"no\"\n",
        "\n",
        "\n",
        "# Reading testing data into a DataFrame mapping them\n",
        "\n",
        "\n",
        "test_data = pd.read_csv(\"test.csv\",header=None)\n",
        "test_data.columns = columns\n",
        "test_data['age'] = test_data['age'].map(num__to_binary_age)\n",
        "test_data['balance'] = test_data['balance'].map(num__to_binary_balance)\n",
        "test_data['day'] = test_data['day'].map(num__to_binary_day)\n",
        "test_data['duration'] = test_data['duration'].map(num__to_binary_duration)\n",
        "test_data['campaign'] = test_data['campaign'].map(num__to_binary_campaign)\n",
        "test_data['pdays'] = test_data['pdays'].map(num__to_binary_pdays)\n",
        "test_data['previous'] = test_data['previous'].map(num__to_binary_previous)\n",
        "X_test = test_data.iloc[:,:-1]\n",
        "Y_test = test_data.iloc[:,-1]\n",
        "\n",
        "\n",
        "# Creating the ID3 Function\n",
        "\n",
        "\n",
        "class ID3:\n",
        "    def __init__(self):\n",
        "        self.data = None\n",
        "        self.features = None\n",
        "        self.labels = None\n",
        "    def total_entropy(self,data,labels):\n",
        "        label_data = data['y'].value_counts()\n",
        "        entropy = 0\n",
        "        for i in label_data:\n",
        "            entropy -= (i/sum(label_data)) * math.log2(i/sum(label_data))\n",
        "        return entropy\n",
        "    def fea_cat_entropy(self,data, feature,labels):\n",
        "        categories_features = data[feature].value_counts().keys()\n",
        "        entropy = 0\n",
        "        for categories in categories_features:\n",
        "            label_fea_data = data[data[feature]==categories]['y'].value_counts()\n",
        "            pd.DataFrame(data)[feature].value_counts()\n",
        "            s = 0\n",
        "            for i in label_fea_data:\n",
        "                s -= (i/sum(label_fea_data)) * math.log2(i/sum(label_fea_data))\n",
        "            entropy += (sum(label_fea_data)/len(data)) * (s)\n",
        "        return entropy\n",
        "    def total_gini(self,data,labels):\n",
        "        label_data = data['y'].value_counts()\n",
        "        entropy = 1\n",
        "        for i in label_data:\n",
        "            entropy -= (i/sum(label_data))**2\n",
        "        return entropy\n",
        "    def fea_cat_gini(self,data, feature,labels):\n",
        "        categories_features = data[feature].value_counts().keys()\n",
        "        entropy = 0\n",
        "        for categories in categories_features:\n",
        "            label_fea_data = data[data[feature]==categories]['y'].value_counts()\n",
        "            pd.DataFrame(data)[feature].value_counts()\n",
        "            s = 1\n",
        "            for i in label_fea_data:\n",
        "                s -= (i/sum(label_fea_data))**2\n",
        "            entropy += (sum(label_fea_data)/len(data)) * (s)\n",
        "        return entropy\n",
        "\n",
        "    def total_me(self,data,labels):\n",
        "        label_data = data['y'].value_counts()\n",
        "        entropy = (min(label_data)/sum(label_data))\n",
        "        return entropy\n",
        "    def fea_cat_me(self,data, feature,labels):\n",
        "        categories_features = data[feature].value_counts().keys()\n",
        "        entropy = 0\n",
        "        for categories in categories_features:\n",
        "            label_fea_data = data[data[feature]==categories]['y'].value_counts()\n",
        "            pd.DataFrame(data)[feature].value_counts()\n",
        "            if len(label_fea_data)!=4:\n",
        "                   entropy+=0\n",
        "            else:\n",
        "                entropy += (min(label_fea_data)/len(data))\n",
        "        return entropy\n",
        "    def IG(self,data,features,labels,split_method):\n",
        "        if split_method==\"entropy\":\n",
        "          return self.total_entropy(data,labels) - self.fea_cat_entropy(data,features,labels)\n",
        "        elif split_method==\"gini\":\n",
        "          return self.total_gini(data,labels) - self.fea_cat_gini(data,features,labels)\n",
        "        elif split_method==\"MajorityError\":\n",
        "          return self.total_me(data,labels) - self.fea_cat_me(data,features,labels)\n",
        "    def create_root_node(self,data,features,split_method,labels):\n",
        "        total_fea_ig = dict()\n",
        "        for i in features:\n",
        "            total_fea_ig[i] = self.IG(data,i,labels,split_method)\n",
        "        best_feature = max(total_fea_ig, key=total_fea_ig.get)\n",
        "        return best_feature\n",
        "    def find_bestsplits(self,data,data_copy,max_depth,depth,root_node):\n",
        "        features_groups = dict()\n",
        "        temp_x = []\n",
        "        if depth<max_depth:\n",
        "          for x in data[root_node].value_counts().keys():\n",
        "              for i in range(len(data)):\n",
        "                  if data.iloc[i][root_node] == x:\n",
        "                      if features_groups.get(x,0)!=0:\n",
        "                          features_groups[x].append((dict(data.iloc[i][:-1]),data.iloc[i]['y']))\n",
        "                      else:\n",
        "                          features_groups[x] = [(dict(data.iloc[i][:-1]),data.iloc[i]['y'])]\n",
        "              temp_x.append(x)\n",
        "          c_ = list(data_copy[root_node].value_counts().keys())\n",
        "          for i in c_:\n",
        "              if i not in temp_x:\n",
        "                label_counts = dict(data['y'].value_counts())\n",
        "                features_groups[i] =  [({}, max(label_counts,key=label_counts.get))]\n",
        "          return features_groups,depth+1\n",
        "        else:\n",
        "          c_ = list(data_copy[root_node].value_counts().keys())\n",
        "          temp_x = list(data[root_node].value_counts().keys())\n",
        "          for i in c_:\n",
        "                if i in temp_x:\n",
        "                  label_counts = dict(data[data[root_node]==i]['y'].value_counts())\n",
        "                  features_groups[i] =  [({}, max(label_counts,key=label_counts.get))]\n",
        "                else:\n",
        "                  label_counts = dict(data['y'].value_counts())\n",
        "                  features_groups[i] =  [({}, max(label_counts,key=label_counts.get))]\n",
        "          return features_groups,depth\n",
        "    def ID3_Algo(self,data,data_copy,features,labels,max_depth,depth,split_method,backup_features=[]):\n",
        "        if len(data['y'].value_counts()) == 1:\n",
        "            return data['y'].value_counts().keys()\n",
        "        if len(features)==0:\n",
        "            label_counts = dict(data['y'].value_counts())\n",
        "            return max(label_counts,key=label_counts.get)\n",
        "        root_node = self.create_root_node(data,features,split_method,labels)\n",
        "\n",
        "        categories = data[root_node].value_counts().keys()\n",
        "        if(root_node in features):\n",
        "            features.remove(root_node)\n",
        "            backup_features.append(root_node)\n",
        "        feature_groups,depth = self.find_bestsplits(data, data_copy,max_depth,depth,root_node)\n",
        "        subtree_dict = {}\n",
        "        final_tree = tuple()\n",
        "        for i ,j in feature_groups.items():\n",
        "            data = pd.DataFrame.from_dict({k: dict(v) for k,v in pd.DataFrame(j)[0].items()}, orient='index')\n",
        "            data['y'] = pd.DataFrame(j)[1]\n",
        "            subtree_dict[i] = self.ID3_Algo(data, data_copy, features,labels,max_depth,depth,split_method,backup_features)\n",
        "            final_tree = (root_node,subtree_dict)\n",
        "        features.append(backup_features[-1])\n",
        "        backup_features.remove(backup_features[-1])\n",
        "        return final_tree\n",
        "def classify(tree, query):\n",
        "      if tree in labels:\n",
        "          return tree\n",
        "      key = query.get(tree[0])\n",
        "      if key not in tree[1]:\n",
        "          key = None\n",
        "      class_ = classify(tree[1][key], query)\n",
        "      return class_\n",
        "best_split = [\"entropy\",\"gini\",\"MajorityError\"]\n",
        "train_error = dict()\n",
        "test_error = dict()\n",
        "\n",
        "for md in range(16):\n",
        "    for j in best_split:\n",
        "        print(\"-\"*5+\" Training Decision Tree with Depth  = \"+str(md+1)+\" and Method of Split as \"+j+\" \"+\"-\"*5)\n",
        "        algo = ID3()\n",
        "        labels = ['yes','no']\n",
        "        answer = dict()\n",
        "        max_depth = md+1\n",
        "        split_method = j\n",
        "        s = algo.ID3_Algo(training_data,training_data,columns[:-1],labels,max_depth,1,split_method,[])\n",
        "        c=0\n",
        "        for i in range(X_train.shape[0]):\n",
        "          sample = dict(X_train.iloc[i])\n",
        "          if classify(s,sample)==Y_train[i]:\n",
        "            c+=1\n",
        "        print(\"Training Missclassified Points(indicating underfitting): \",(X_train.shape[0]-c))\n",
        "        print(\"Training Error\",(X_train.shape[0]-c)/X_train.shape[0])\n",
        "        if train_error.get(max_depth):\n",
        "            train_error[max_depth].append((j,(X_train.shape[0]-c)/X_train.shape[0]))\n",
        "        else:\n",
        "            train_error[max_depth] = [(j,(X_train.shape[0]-c)/X_train.shape[0])]\n",
        "        c=0\n",
        "        for i in range(X_test.shape[0]):\n",
        "          sample = dict(X_test.iloc[i])\n",
        "          if classify(s,sample)==Y_test[i]:\n",
        "            c+=1\n",
        "        print(\"Testing Missclassified Points(indicating overfitting): \",(X_test.shape[0]-c))\n",
        "        print(\"Testing Error: \",(X_test.shape[0]-c)/X_test.shape[0])\n",
        "        if test_error.get(max_depth):\n",
        "            test_error[max_depth] .append((j,(X_test.shape[0]-c)/X_test.shape[0]))\n",
        "        else:\n",
        "            test_error[max_depth] = [(j,(X_test.shape[0]-c)/X_test.shape[0])]\n",
        "        print(\"*\"*50)"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "DNu6noaCWqhC"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}