# -*- coding: utf-8 -*-
"""ML_Solution_3(a)

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1_FUoOsmWpHgxxCkGbtLz7AbtFSAkMIJJ
"""

import numpy as np
import math
import pandas as pd
#Created Variable training_data to read the content of train csv
training_data = pd.read_csv("train.csv",header=None)


print("Solution for Part 3(a) BELOW")


training_data_backup = training_data.copy()

#Columns present inside data-desc file
columns = ["age","job","marital","education","default","balance","housing","loan","contact","day","month","duration","campaign","pdays","previous","poutcome","y"]


# Mapping Columns
training_data.columns = columns
training_data_backup.columns = columns

def num__to_binary_age(x):
    if x>np.median(training_data['age']):
        return "yes"
    else:
        return "no"
def num__to_binary_balance(x):
    if x>np.median(training_data['balance']):
        return "yes"
    else:
        return "no"
def num__to_binary_day(x):
    if x>np.median(training_data['day']):
        return "yes"
    else:
        return "no"
def num__to_binary_duration(x):
    if x>np.median(training_data['duration']):
        return "yes"
    else:
        return "no"
def num__to_binary_campaign(x):
    if x>np.median(training_data['campaign']):
        return "yes"
    else:
        return "no"
def num__to_binary_pdays(x):
    if x>np.median(training_data['pdays']):
        return "yes"
    else:
        return "no"
def num__to_binary_previous(x):
    if x>np.median(training_data['previous']):
        return "yes"
    else:
        return "no"


# Mapping Above created Function


training_data['age'] = training_data['age'].map(num__to_binary_age)
training_data['balance'] = training_data['balance'].map(num__to_binary_balance)
training_data['day'] = training_data['day'].map(num__to_binary_day)
training_data['duration'] = training_data['duration'].map(num__to_binary_duration)
training_data['campaign'] = training_data['campaign'].map(num__to_binary_campaign)
training_data['pdays'] = training_data['pdays'].map(num__to_binary_pdays)
training_data['previous'] = training_data['previous'].map(num__to_binary_previous)


# Training the data along x and y direction.


X_train = training_data.iloc[:,:-1]
Y_train = training_data.iloc[:,-1]


def num__to_binary_age(x):
    if x>np.median(training_data_backup['age']):
        return "yes"
    else:
        return "no"
def num__to_binary_balance(x):
    if x>np.median(training_data_backup['balance']):
        return "yes"
    else:
        return "no"
def num__to_binary_day(x):
    if x>np.median(training_data_backup['day']):
        return "yes"
    else:
        return "no"
def num__to_binary_duration(x):
    if x>np.median(training_data_backup['duration']):
        return "yes"
    else:
        return "no"
def num__to_binary_campaign(x):
    if x>np.median(training_data_backup['campaign']):
        return "yes"
    else:
        return "no"
def num__to_binary_pdays(x):
    if x>np.median(training_data_backup['pdays']):
        return "yes"
    else:
        return "no"
def num__to_binary_previous(x):
    if x>np.median(training_data_backup['previous']):
        return "yes"
    else:
        return "no"


# Reading testing data into a DataFrame mapping them


test_data = pd.read_csv("test.csv",header=None)
test_data.columns = columns
test_data['age'] = test_data['age'].map(num__to_binary_age)
test_data['balance'] = test_data['balance'].map(num__to_binary_balance)
test_data['day'] = test_data['day'].map(num__to_binary_day)
test_data['duration'] = test_data['duration'].map(num__to_binary_duration)
test_data['campaign'] = test_data['campaign'].map(num__to_binary_campaign)
test_data['pdays'] = test_data['pdays'].map(num__to_binary_pdays)
test_data['previous'] = test_data['previous'].map(num__to_binary_previous)
X_test = test_data.iloc[:,:-1]
Y_test = test_data.iloc[:,-1]


# Creating the ID3 Function


class ID3:
    def __init__(self):
        self.data = None
        self.features = None
        self.labels = None
    def total_entropy(self,data,labels):
        label_data = data['y'].value_counts()
        entropy = 0
        for i in label_data:
            entropy -= (i/sum(label_data)) * math.log2(i/sum(label_data))
        return entropy
    def fea_cat_entropy(self,data, feature,labels):
        categories_features = data[feature].value_counts().keys()
        entropy = 0
        for categories in categories_features:
            label_fea_data = data[data[feature]==categories]['y'].value_counts()
            pd.DataFrame(data)[feature].value_counts()
            s = 0
            for i in label_fea_data:
                s -= (i/sum(label_fea_data)) * math.log2(i/sum(label_fea_data))
            entropy += (sum(label_fea_data)/len(data)) * (s)
        return entropy
    def total_gini(self,data,labels):
        label_data = data['y'].value_counts()
        entropy = 1
        for i in label_data:
            entropy -= (i/sum(label_data))**2
        return entropy
    def fea_cat_gini(self,data, feature,labels):
        categories_features = data[feature].value_counts().keys()
        entropy = 0
        for categories in categories_features:
            label_fea_data = data[data[feature]==categories]['y'].value_counts()
            pd.DataFrame(data)[feature].value_counts()
            s = 1
            for i in label_fea_data:
                s -= (i/sum(label_fea_data))**2
            entropy += (sum(label_fea_data)/len(data)) * (s)
        return entropy

    def total_me(self,data,labels):
        label_data = data['y'].value_counts()
        entropy = (min(label_data)/sum(label_data))
        return entropy
    def fea_cat_me(self,data, feature,labels):
        categories_features = data[feature].value_counts().keys()
        entropy = 0
        for categories in categories_features:
            label_fea_data = data[data[feature]==categories]['y'].value_counts()
            pd.DataFrame(data)[feature].value_counts()
            if len(label_fea_data)!=4:
                   entropy+=0
            else:
                entropy += (min(label_fea_data)/len(data))
        return entropy
    def IG(self,data,features,labels,split_method):
        if split_method=="entropy":
          return self.total_entropy(data,labels) - self.fea_cat_entropy(data,features,labels)
        elif split_method=="gini":
          return self.total_gini(data,labels) - self.fea_cat_gini(data,features,labels)
        elif split_method=="MajorityError":
          return self.total_me(data,labels) - self.fea_cat_me(data,features,labels)
    def create_root_node(self,data,features,split_method,labels):
        total_fea_ig = dict()
        for i in features:
            total_fea_ig[i] = self.IG(data,i,labels,split_method)
        best_feature = max(total_fea_ig, key=total_fea_ig.get)
        return best_feature
    def find_bestsplits(self,data,data_copy,max_depth,depth,root_node):
        features_groups = dict()
        temp_x = []
        if depth<max_depth:
          for x in data[root_node].value_counts().keys():
              for i in range(len(data)):
                  if data.iloc[i][root_node] == x:
                      if features_groups.get(x,0)!=0:
                          features_groups[x].append((dict(data.iloc[i][:-1]),data.iloc[i]['y']))
                      else:
                          features_groups[x] = [(dict(data.iloc[i][:-1]),data.iloc[i]['y'])]
              temp_x.append(x)
          c_ = list(data_copy[root_node].value_counts().keys())
          for i in c_:
              if i not in temp_x:
                label_counts = dict(data['y'].value_counts())
                features_groups[i] =  [({}, max(label_counts,key=label_counts.get))]
          return features_groups,depth+1
        else:
          c_ = list(data_copy[root_node].value_counts().keys())
          temp_x = list(data[root_node].value_counts().keys())
          for i in c_:
                if i in temp_x:
                  label_counts = dict(data[data[root_node]==i]['y'].value_counts())
                  features_groups[i] =  [({}, max(label_counts,key=label_counts.get))]
                else:
                  label_counts = dict(data['y'].value_counts())
                  features_groups[i] =  [({}, max(label_counts,key=label_counts.get))]
          return features_groups,depth
    def ID3_Algo(self,data,data_copy,features,labels,max_depth,depth,split_method,backup_features=[]):
        if len(data['y'].value_counts()) == 1:
            return data['y'].value_counts().keys()
        if len(features)==0:
            label_counts = dict(data['y'].value_counts())
            return max(label_counts,key=label_counts.get)
        root_node = self.create_root_node(data,features,split_method,labels)

        categories = data[root_node].value_counts().keys()
        if(root_node in features):
            features.remove(root_node)
            backup_features.append(root_node)
        feature_groups,depth = self.find_bestsplits(data, data_copy,max_depth,depth,root_node)
        subtree_dict = {}
        final_tree = tuple()
        for i ,j in feature_groups.items():
            data = pd.DataFrame.from_dict({k: dict(v) for k,v in pd.DataFrame(j)[0].items()}, orient='index')
            data['y'] = pd.DataFrame(j)[1]
            subtree_dict[i] = self.ID3_Algo(data, data_copy, features,labels,max_depth,depth,split_method,backup_features)
            final_tree = (root_node,subtree_dict)
        features.append(backup_features[-1])
        backup_features.remove(backup_features[-1])
        return final_tree
def classify(tree, query):
      if tree in labels:
          return tree
      key = query.get(tree[0])
      if key not in tree[1]:
          key = None
      class_ = classify(tree[1][key], query)
      return class_
best_split = ["entropy","gini","MajorityError"]
train_error = dict()
test_error = dict()

for md in range(16):
    for j in best_split:
        print("-"*5+" Training Decision Tree with Depth  = "+str(md+1)+" and Method of Split as "+j+" "+"-"*5)
        algo = ID3()
        labels = ['yes','no']
        answer = dict()
        max_depth = md+1
        split_method = j
        s = algo.ID3_Algo(training_data,training_data,columns[:-1],labels,max_depth,1,split_method,[])
        c=0
        for i in range(X_train.shape[0]):
          sample = dict(X_train.iloc[i])
          if classify(s,sample)==Y_train[i]:
            c+=1
        print("Training Missclassified Points(indicating underfitting): ",(X_train.shape[0]-c))
        print("Training Error",(X_train.shape[0]-c)/X_train.shape[0])
        if train_error.get(max_depth):
            train_error[max_depth].append((j,(X_train.shape[0]-c)/X_train.shape[0]))
        else:
            train_error[max_depth] = [(j,(X_train.shape[0]-c)/X_train.shape[0])]
        c=0
        for i in range(X_test.shape[0]):
          sample = dict(X_test.iloc[i])
          if classify(s,sample)==Y_test[i]:
            c+=1
        print("Testing Missclassified Points(indicating overfitting): ",(X_test.shape[0]-c))
        print("Testing Error: ",(X_test.shape[0]-c)/X_test.shape[0])
        if test_error.get(max_depth):
            test_error[max_depth] .append((j,(X_test.shape[0]-c)/X_test.shape[0]))
        else:
            test_error[max_depth] = [(j,(X_test.shape[0]-c)/X_test.shape[0])]
        print("*"*50)

