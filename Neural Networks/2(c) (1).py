# -*- coding: utf-8 -*-
"""2(c)

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1uMNh5jM39i7oXFXsKWXwhSes4PM15eJJ
"""

# Importing all the important library
import numpy as np
import pandas as pd

# Sigmoid activation function and its gradient
def sigmoid(x):
    return 1 / (1 + np.exp(-x))

def sigmoid_gradient(x):
    s = sigmoid(x)
    return s * (1 - s)

# Binary cross-entropy loss for classification
def binary_cross_entropy_loss(y_true, y_pred):
    epsilon = 1e-8  # Small constant to prevent log(0)
    return -np.mean(y_true * np.log(y_pred + epsilon) + (1 - y_true) * np.log(1 - y_pred + epsilon))

# Load the dataset and normalize features
def load_and_normalize(train_file, test_file):
    train_data = pd.read_csv(train_file, header=None)
    test_data = pd.read_csv(test_file, header=None)

    X_train, y_train = train_data.iloc[:, :-1].values, train_data.iloc[:, -1].values
    X_test, y_test = test_data.iloc[:, :-1].values, test_data.iloc[:, -1].values

    # Standardize the features (mean = 0, std = 1)
    mean = np.mean(X_train, axis=0)
    std = np.std(X_train, axis=0)
    X_train = (X_train - mean) / std
    X_test = (X_test - mean) / std

    return X_train, y_train, X_test, y_test

# Initialize weights and biases with zeros
def initialize_params_zero(input_dim, hidden_dim, output_dim):
    params = {
        "W1": np.zeros((input_dim, hidden_dim)),
        "b1": np.zeros((1, hidden_dim)),
        "W2": np.zeros((hidden_dim, hidden_dim)),
        "b2": np.zeros((1, hidden_dim)),
        "W3": np.zeros((hidden_dim, output_dim)),
        "b3": np.zeros((1, output_dim)),
    }
    return params

# Perform a forward pass through the network
def forward(X, params):
    Z1 = np.dot(X, params["W1"]) + params["b1"]
    A1 = sigmoid(Z1)
    Z2 = np.dot(A1, params["W2"]) + params["b2"]
    A2 = sigmoid(Z2)
    Z3 = np.dot(A2, params["W3"]) + params["b3"]
    A3 = sigmoid(Z3)
    cache = {"Z1": Z1, "A1": A1, "Z2": Z2, "A2": A2, "Z3": Z3, "A3": A3}
    return A3, cache

# Perform backpropagation to calculate gradients
def backward(y, params, cache):
    A3, A2, A1 = cache["A3"], cache["A2"], cache["A1"]
    Z2, Z1 = cache["Z2"], cache["Z1"]
    X = cache["X"]

    dZ3 = A3 - y.reshape(-1, 1)
    dW3 = np.dot(A2.T, dZ3)
    db3 = np.sum(dZ3, axis=0, keepdims=True)

    dZ2 = np.dot(dZ3, params["W3"].T) * sigmoid_gradient(Z2)
    dW2 = np.dot(A1.T, dZ2)
    db2 = np.sum(dZ2, axis=0, keepdims=True)

    dZ1 = np.dot(dZ2, params["W2"].T) * sigmoid_gradient(Z1)
    dW1 = np.dot(X.T, dZ1)
    db1 = np.sum(dZ1, axis=0, keepdims=True)

    gradients = {"dW3": dW3, "db3": db3, "dW2": dW2, "db2": db2, "dW1": dW1, "db1": db1}
    return gradients

# Update parameters using gradients
def update_params(params, gradients, learning_rate):
    for key in params.keys():
        params[key] -= learning_rate * gradients["d" + key]
    return params

# Learning rate scheduler based on time
def lr_scheduler(iteration, initial_lr, decay_factor):
    return initial_lr / (1 + initial_lr / decay_factor * iteration)

# Train the neural network with stochastic gradient descent
def train_nn(X_train, y_train, X_test, y_test, hidden_dim, initial_lr, decay_factor, epochs=50):
    input_dim = X_train.shape[1]
    output_dim = 1
    # Initialize weights to zero
    params = initialize_params_zero(input_dim, hidden_dim, output_dim)
    n_samples = X_train.shape[0]

    results = []

    for epoch in range(epochs):
        # Shuffle training data
        indices = np.arange(n_samples)
        np.random.shuffle(indices)
        X_train = X_train[indices]
        y_train = y_train[indices]

        for step, (x, y) in enumerate(zip(X_train, y_train), start=1):
            x = x.reshape(1, -1)
            y = np.array([y])

            # Forward pass
            A3, cache = forward(x, params)

            # Backward pass
            cache["X"] = x
            gradients = backward(y, params, cache)

            # Update parameters
            lr = lr_scheduler(step, initial_lr, decay_factor)
            params = update_params(params, gradients, lr)

        # Compute train and test errors
        A3_train, _ = forward(X_train, params)
        train_loss = binary_cross_entropy_loss(y_train, A3_train)

        A3_test, _ = forward(X_test, params)
        test_loss = binary_cross_entropy_loss(y_test, A3_test)

        print(f"Epoch {epoch+1}, Train Error: {train_loss:.6f}, Test Error: {test_loss:.6f}")

    # Compute final accuracy on test data
    y_pred = (A3_test > 0.5).astype(int)
    accuracy = np.mean(y_pred == y_test.reshape(-1, 1))
    results.append((hidden_dim, test_loss, accuracy * 100))
    return results

# Main function to execute the code
if __name__ == "__main__":
    # Load the dataset
    X_train, y_train, X_test, y_test = load_and_normalize("train.csv", "test.csv")

    initial_lr = 0.1
    decay_factor = 0.1
    epochs = 100

    all_results = []

    for hidden_dim in [5, 10, 25, 50, 100]:
        print(f"\nTraining with Width: {hidden_dim}")
        results = train_nn(X_train, y_train, X_test, y_test, hidden_dim, initial_lr, decay_factor, epochs)
        all_results.extend(results)

    # Display results in a table
    print("\nFinal Results:")
    print("Hidden Layer Size | Test Loss | Accuracy (%)")
    for hidden_dim, test_loss, accuracy in all_results:
        print(f"{hidden_dim:<18} | {test_loss:.6f}    | {accuracy:.2f}")

