# -*- coding: utf-8 -*-
"""2(e)

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1uMNh5jM39i7oXFXsKWXwhSes4PM15eJJ
"""

# Import necessary libraries
import torch
import torch.nn as nn
import torch.optim as optim
import numpy as np
import pandas as pd

# Load and prepare the dataset
def load_dataset(training_path, testing_path):
    # Read the CSV files
    train_set = pd.read_csv(training_path, header=None).values
    test_set = pd.read_csv(testing_path, header=None).values

    # Separate features and labels
    train_features, train_labels = train_set[:, :-1], train_set[:, -1]
    test_features, test_labels = test_set[:, :-1], test_set[:, -1]

    # Normalize the features
    feature_mean = np.mean(train_features, axis=0)
    feature_std = np.std(train_features, axis=0)

    train_features = (train_features - feature_mean) / feature_std
    test_features = (test_features - feature_mean) / feature_std

    # We then Convert to PyTorch tensors
    train_features = torch.tensor(train_features, dtype=torch.float32)
    train_labels = torch.tensor(train_labels, dtype=torch.float32)
    test_features = torch.tensor(test_features, dtype=torch.float32)
    test_labels = torch.tensor(test_labels, dtype=torch.float32)

    return train_features, train_labels, test_features, test_labels

# Define the neural network
class CustomNeuralNetwork(nn.Module):
    def __init__(self, input_dim, hidden_dims, output_dim, activation_choice, weight_init):
        super(CustomNeuralNetwork, self).__init__()
        self.hidden_layers = nn.ModuleList()
        previous_dim = input_dim

        # Add hidden layers
        for hidden_dim in hidden_dims:
            layer = nn.Linear(previous_dim, hidden_dim)
            if weight_init == "xavier":
                nn.init.xavier_uniform_(layer.weight)
            elif weight_init == "he":
                nn.init.kaiming_uniform_(layer.weight, nonlinearity='relu')
            nn.init.zeros_(layer.bias)
            self.hidden_layers.append(layer)
            previous_dim = hidden_dim

        # Add output layer
        self.final_layer = nn.Linear(previous_dim, output_dim)
        nn.init.xavier_uniform_(self.final_layer.weight)
        nn.init.zeros_(self.final_layer.bias)

        # Choose activation function
        self.activation = nn.Tanh() if activation_choice == "tanh" else nn.ReLU()

    def forward(self, x):
        for layer in self.hidden_layers:
            x = self.activation(layer(x))
        x = torch.sigmoid(self.final_layer(x))  # Use sigmoid for binary classification
        return x

# Function for training and testing
def train_and_test(features_train, labels_train, features_test, labels_test, hidden_dims, activation_choice, weight_init, num_layers, layer_width, num_epochs=50):
    input_dim = features_train.shape[1]
    output_dim = 1  # Binary classification

    # Create the model
    neural_model = CustomNeuralNetwork(input_dim, hidden_dims, output_dim, activation_choice, weight_init)
    loss_function = nn.BCELoss()  # Binary cross-entropy loss
    optimizer = optim.Adam(neural_model.parameters(), lr=0.001)

    # Train the model
    for epoch in range(num_epochs):
        neural_model.train()
        optimizer.zero_grad()
        predictions_train = neural_model(features_train).squeeze()
        train_loss = loss_function(predictions_train, labels_train)
        train_loss.backward()
        optimizer.step()

        # Test the model
        neural_model.eval()
        with torch.no_grad():
            predictions_test = neural_model(features_test).squeeze()
            test_loss = loss_function(predictions_test, labels_test)

        print(f"Value Epoch {epoch + 1}, Train Error: {train_loss.item():.6f}, Test Error: {test_loss.item():.6f}")

    # Calculate test accuracy
    with torch.no_grad():
        final_predictions = (predictions_test > 0.5).float()
        test_accuracy = (final_predictions == labels_test).float().mean().item()
        print(f"Layers: {num_layers}, Width: {layer_width}, Activation: {activation_choice}, Init: {weight_init}, Accuracy: {test_accuracy * 100:.6f}%\n")

# Main function
if __name__ == "__main__":
    # Load the dataset
    features_train, labels_train, features_test, labels_test = load_dataset("train.csv", "test.csv")

    # Configurations for testing
    layer_counts = [3, 5, 9]
    layer_widths = [5, 10, 25, 50, 100]
    activation_functions = ["tanh", "relu"]
    initialization_methods = ["xavier", "he"]

    # Run the experiments
    for activation in activation_functions:
        for init in initialization_methods:
            for layers in layer_counts:
                for width in layer_widths:
                    hidden_dims = [width] * layers  # Set the same width for all layers
                    print(f"Training: Layers={layers}, Width={width}, Activation={activation}, Init={init}")
                    train_and_test(features_train, labels_train, features_test, labels_test, hidden_dims, activation, init, layers, width)

