# -*- coding: utf-8 -*-
"""2(a)

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1uMNh5jM39i7oXFXsKWXwhSes4PM15eJJ
"""

import numpy as np
import pandas as pd
from tabulate import tabulate

# Sigmoid function for activation and its derivative for backpropagation
def activation(x):
    return 1 / (1 + np.exp(-x))

def activation_derivative(x):
    return activation(x) * (1 - activation(x))

# Binary cross-entropy loss for evaluating predictions
def calculate_loss(y_actual, y_predicted):
    return -np.mean(y_actual * np.log(y_predicted + 1e-8) + (1 - y_actual) * np.log(1 - y_predicted + 1e-8))

# Load dataset from CSV files
def load_dataset(training_file, testing_file):
    training_data = pd.read_csv(training_file, header=None)
    testing_data = pd.read_csv(testing_file, header=None)
    X_train, y_train = training_data.iloc[:, :-1].values, training_data.iloc[:, -1].values
    X_test, y_test = testing_data.iloc[:, :-1].values, testing_data.iloc[:, -1].values
    return X_train, y_train, X_test, y_test

# Initialize weights and biases for the network
def initialize_parameters(input_dim, hidden1_dim, hidden2_dim, output_dim):
    np.random.seed(42)  # For consistent results
    parameters = {
        "W1": np.random.randn(input_dim, hidden1_dim) * 0.1,
        "b1": np.zeros((1, hidden1_dim)),
        "W2": np.random.randn(hidden1_dim, hidden2_dim) * 0.1,
        "b2": np.zeros((1, hidden2_dim)),
        "W3": np.random.randn(hidden2_dim, output_dim) * 0.1,
        "b3": np.zeros((1, output_dim)),
    }
    return parameters

# Perform a forward pass through the network
def forward_propagation(inputs, parameters):
    Z1 = np.dot(inputs, parameters["W1"]) + parameters["b1"]
    A1 = activation(Z1)

    Z2 = np.dot(A1, parameters["W2"]) + parameters["b2"]
    A2 = activation(Z2)

    Z3 = np.dot(A2, parameters["W3"]) + parameters["b3"]
    A3 = activation(Z3)  # Final output layer

    cache = {"inputs": inputs, "Z1": Z1, "A1": A1, "Z2": Z2, "A2": A2, "Z3": Z3, "A3": A3}
    return A3, cache

# Perform a backward pass to compute gradients
def backward_propagation(targets, parameters, cache):
    num_samples = targets.shape[0]
    A3, A2, A1, inputs = cache["A3"], cache["A2"], cache["A1"], cache["inputs"]
    Z2, Z1 = cache["Z2"], cache["Z1"]

    # Gradients for the output layer
    dZ3 = A3 - targets.reshape(-1, 1)
    dW3 = np.dot(A2.T, dZ3) / num_samples
    db3 = np.sum(dZ3, axis=0, keepdims=True) / num_samples

    # Gradients for the second hidden layer
    dZ2 = np.dot(dZ3, parameters["W3"].T) * activation_derivative(Z2)
    dW2 = np.dot(A1.T, dZ2) / num_samples
    db2 = np.sum(dZ2, axis=0, keepdims=True) / num_samples

    # Gradients for the first hidden layer
    dZ1 = np.dot(dZ2, parameters["W2"].T) * activation_derivative(Z1)
    dW1 = np.dot(inputs.T, dZ1) / num_samples
    db1 = np.sum(dZ1, axis=0, keepdims=True) / num_samples

    gradients = {"dW3": dW3, "db3": db3, "dW2": dW2, "db2": db2, "dW1": dW1, "db1": db1}
    return gradients

# Update weights and biases using gradient descent
def update_parameters(parameters, gradients, lr):
    for key in parameters.keys():
        parameters[key] -= lr * gradients["d" + key]
    return parameters

# Train the network and display progress
def train_network(train_inputs, train_targets, test_inputs, test_targets, hidden1_dim=10, hidden2_dim=8, epochs=1000, lr=0.01):
    input_dim = train_inputs.shape[1]
    output_dim = 1  # Binary output
    parameters = initialize_parameters(input_dim, hidden1_dim, hidden2_dim, output_dim)

    loss_table = []

    for epoch in range(epochs):
        # Forward pass
        predictions, cache = forward_propagation(train_inputs, parameters)
        train_loss = calculate_loss(train_targets, predictions)

        # Backward pass
        gradients = backward_propagation(train_targets, parameters, cache)

        # Update weights
        parameters = update_parameters(parameters, gradients, lr)

        # Evaluate on test set every 100 epochs
        if epoch % 100 == 0:
            test_predictions, _ = forward_propagation(test_inputs, parameters)
            test_loss = calculate_loss(test_targets, test_predictions)
            loss_table.append([epoch, train_loss, test_loss])
            # print("In Epoch block")
            #print(f"Epoch {epoch}, Train Loss: {train_loss:.4f}, Test Loss: {test_loss:.4f}")

    # Display results as a table for better visualization
    print("\nTraining Progress:")
    print(tabulate(loss_table, headers=["Epoch", "Train Loss", "Test Loss"], floatfmt=".4f"))
    #print("STARTED...")
    # Evaluate accuracy on test data
    final_predictions, _ = forward_propagation(test_inputs, parameters)
    binary_predictions = (final_predictions > 0.5).astype(int)
    accuracy = np.mean(binary_predictions == test_targets.reshape(-1, 1)) * 100
    print(f"\nTest Accuracy: {accuracy:.2f}%")

# Main function
if __name__ == "__main__":
    # Load the data defined above
    train_inputs, train_targets, test_inputs, test_targets = load_dataset("train.csv", "test.csv")

    # Train the neural network
    train_network(train_inputs, train_targets, test_inputs, test_targets, hidden1_dim=20, hidden2_dim=8, epochs=2000, lr=0.05)

