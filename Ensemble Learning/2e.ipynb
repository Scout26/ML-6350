{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0N0SJCde2s8O",
        "outputId": "3ed64e53-c900-4bdd-df78-4e48c1494eff"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Single Tree - Bias^2: 0.341472, Variance: 0.303968, Total Error: 0.64544\n",
            "Bagged Trees - Bias^2: 0.30278796, Variance: 0.01485884, Total Error: 0.3176468\n",
            "Single Random Tree - Bias^2: 0.343608, Variance: 0.30431199999999997, Total Error: 0.64792\n",
            "Random Forest - Bias^2: 0.30346629999999997, Variance: 0.0164491, Total Error: 0.31991539999999996\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from collections import Counter\n",
        "\n",
        "# Function to extract a random subset of data\n",
        "def extract_subset(features, labels, subset_size=1000):\n",
        "    idx = np.random.choice(features.shape[0], subset_size, replace=False)\n",
        "    return features[idx], labels[idx]\n",
        "\n",
        "# Function to initialize a decision tree node\n",
        "def init_node(label=None):\n",
        "    return {'label': label, 'left_child': None, 'right_child': None, 'split_feature': None, 'split_value': None}\n",
        "\n",
        "# Function to check if a node is terminal\n",
        "def is_terminal(node):\n",
        "    return node['label'] is not None\n",
        "\n",
        "# Function to categorize features\n",
        "def categorize_features(features):\n",
        "    feature_categories = []\n",
        "    for i in range(features.shape[1]):\n",
        "        distinct_values = np.unique(features[:, i])\n",
        "        if isinstance(distinct_values[0], (int, float)) and len(distinct_values) > 10:\n",
        "            feature_categories.append(\"continuous\")\n",
        "        else:\n",
        "            feature_categories.append(\"discrete\")\n",
        "    return feature_categories\n",
        "\n",
        "# Function to partition data based on a feature and threshold\n",
        "def partition_data(features, split_feature, split_value):\n",
        "    left_subset = np.where(features[:, split_feature] <= split_value)[0]\n",
        "    right_subset = np.where(features[:, split_feature] > split_value)[0]\n",
        "    return left_subset, right_subset\n",
        "\n",
        "# Function to compute entropy\n",
        "def compute_entropy(labels):\n",
        "    label_freq = Counter(labels)\n",
        "    total_count = len(labels)\n",
        "    entropy = 0.0\n",
        "    for freq in label_freq.values():\n",
        "        prob = freq / total_count\n",
        "        if prob > 0:\n",
        "            entropy -= prob * np.log2(prob)\n",
        "    return entropy\n",
        "\n",
        "# Function to calculate information gain\n",
        "def calc_info_gain(parent_labels, left_labels, right_labels):\n",
        "    n = len(parent_labels)\n",
        "    parent_entropy = compute_entropy(parent_labels)\n",
        "    left_entropy = compute_entropy(left_labels)\n",
        "    right_entropy = compute_entropy(right_labels)\n",
        "    weighted_child_entropy = (len(left_labels) / n) * left_entropy + (len(right_labels) / n) * right_entropy\n",
        "    return parent_entropy - weighted_child_entropy\n",
        "\n",
        "# Function to determine the optimal split\n",
        "def find_best_split(features, labels, feature_categories):\n",
        "    max_gain = -np.inf\n",
        "    optimal_feature = None\n",
        "    optimal_value = None\n",
        "    num_features = features.shape[1]\n",
        "\n",
        "    for feature in range(num_features):\n",
        "        if feature_categories[feature] == \"continuous\":\n",
        "            potential_splits = np.unique(features[:, feature])\n",
        "            for split in potential_splits:\n",
        "                left_idx, right_idx = partition_data(features, feature, split)\n",
        "                if len(left_idx) == 0 or len(right_idx) == 0:\n",
        "                    continue\n",
        "                gain = calc_info_gain(labels, labels[left_idx], labels[right_idx])\n",
        "                if gain > max_gain:\n",
        "                    max_gain = gain\n",
        "                    optimal_feature = feature\n",
        "                    optimal_value = split\n",
        "        else:\n",
        "            distinct_values = np.unique(features[:, feature])\n",
        "            for value in distinct_values:\n",
        "                left_idx, right_idx = partition_data(features, feature, value)\n",
        "                if len(left_idx) == 0 or len(right_idx) == 0:\n",
        "                    continue\n",
        "                gain = calc_info_gain(labels, labels[left_idx], labels[right_idx])\n",
        "                if gain > max_gain:\n",
        "                    max_gain = gain\n",
        "                    optimal_feature = feature\n",
        "                    optimal_value = value\n",
        "\n",
        "    return optimal_feature, optimal_value\n",
        "\n",
        "# Function to construct a decision tree\n",
        "def construct_tree(features, labels, feature_categories, max_features=None):\n",
        "    if len(np.unique(labels)) == 1:\n",
        "        return init_node(label=labels[0])\n",
        "\n",
        "    if max_features is not None:\n",
        "        feature_subset = np.random.choice(len(feature_categories), max_features, replace=False)\n",
        "    else:\n",
        "        feature_subset = range(len(feature_categories))\n",
        "\n",
        "    max_gain = -np.inf\n",
        "    optimal_feature = None\n",
        "    optimal_value = None\n",
        "\n",
        "    for feature in feature_subset:\n",
        "        if feature_categories[feature] == \"continuous\":\n",
        "            potential_splits = np.unique(features[:, feature])\n",
        "            for split in potential_splits:\n",
        "                left_idx, right_idx = partition_data(features, feature, split)\n",
        "                if len(left_idx) == 0 or len(right_idx) == 0:\n",
        "                    continue\n",
        "                gain = calc_info_gain(labels, labels[left_idx], labels[right_idx])\n",
        "                if gain > max_gain:\n",
        "                    max_gain = gain\n",
        "                    optimal_feature = feature\n",
        "                    optimal_value = split\n",
        "        else:\n",
        "            distinct_values = np.unique(features[:, feature])\n",
        "            for value in distinct_values:\n",
        "                left_idx, right_idx = partition_data(features, feature, value)\n",
        "                if len(left_idx) == 0 or len(right_idx) == 0:\n",
        "                    continue\n",
        "                gain = calc_info_gain(labels, labels[left_idx], labels[right_idx])\n",
        "                if gain > max_gain:\n",
        "                    max_gain = gain\n",
        "                    optimal_feature = feature\n",
        "                    optimal_value = value\n",
        "\n",
        "    if optimal_feature is None:\n",
        "        return init_node(label=Counter(labels).most_common(1)[0][0])\n",
        "\n",
        "    left_idx, right_idx = partition_data(features, optimal_feature, optimal_value)\n",
        "\n",
        "    if len(left_idx) == 0 or len(right_idx) == 0:\n",
        "        return init_node(label=Counter(labels).most_common(1)[0][0])\n",
        "\n",
        "    left_subtree = construct_tree(features[left_idx], labels[left_idx], feature_categories, max_features)\n",
        "    right_subtree = construct_tree(features[right_idx], labels[right_idx], feature_categories, max_features)\n",
        "\n",
        "    root = init_node()\n",
        "    root['split_feature'] = optimal_feature\n",
        "    root['split_value'] = optimal_value\n",
        "    root['left_child'] = left_subtree\n",
        "    root['right_child'] = right_subtree\n",
        "\n",
        "    return root\n",
        "\n",
        "# Function to make predictions using a decision tree\n",
        "def tree_predict(features, tree):\n",
        "    predictions = np.zeros(features.shape[0])\n",
        "    for i, instance in enumerate(features):\n",
        "        node = tree\n",
        "        while not is_terminal(node):\n",
        "            if instance[node['split_feature']] <= node['split_value']:\n",
        "                node = node['left_child']\n",
        "            else:\n",
        "                node = node['right_child']\n",
        "        predictions[i] = node['label']\n",
        "    return predictions\n",
        "\n",
        "# Main function to implement bagging and random forest with bias-variance analysis\n",
        "def ensemble_learning(train_features, train_labels, test_features, test_labels, num_iterations=10, num_trees=20, subset_size=300):\n",
        "    single_tree_predictions = np.zeros((num_iterations, test_features.shape[0]))\n",
        "    bagged_predictions = np.zeros((num_iterations, test_features.shape[0]))\n",
        "\n",
        "    feature_categories = categorize_features(train_features)\n",
        "\n",
        "    for i in range(num_iterations):\n",
        "        # Extract a subset of data\n",
        "        features_subset, labels_subset = extract_subset(train_features, train_labels, subset_size)\n",
        "\n",
        "        # Train a single decision tree\n",
        "        single_tree = construct_tree(features_subset, labels_subset, feature_categories)\n",
        "        single_tree_predictions[i] = tree_predict(test_features, single_tree)\n",
        "\n",
        "        # Train an ensemble of decision trees\n",
        "        ensemble_predictions = np.zeros((num_trees, test_features.shape[0]))\n",
        "        for t in range(num_trees):\n",
        "            features_bag, labels_bag = extract_subset(train_features, train_labels, subset_size)\n",
        "            tree = construct_tree(features_bag, labels_bag, feature_categories)\n",
        "            ensemble_predictions[t] = tree_predict(test_features, tree)\n",
        "\n",
        "        # Aggregate predictions for bagged trees\n",
        "        bagged_predictions[i] = np.mean(ensemble_predictions, axis=0)\n",
        "\n",
        "    # Compute bias and variance\n",
        "    single_tree_bias, single_tree_variance = compute_bias_variance(single_tree_predictions, test_labels)\n",
        "    bagged_tree_bias, bagged_tree_variance = compute_bias_variance(bagged_predictions, test_labels)\n",
        "\n",
        "    return single_tree_bias, single_tree_variance, bagged_tree_bias, bagged_tree_variance\n",
        "\n",
        "# Main function to implement random forest with bias-variance analysis\n",
        "def random_forest_learning(train_features, train_labels, test_features, test_labels, num_iterations=10, num_trees=20, subset_size=300, max_features=4):\n",
        "    rf_single_tree_predictions = np.zeros((num_iterations, test_features.shape[0]))\n",
        "    forest_predictions = np.zeros((num_iterations, test_features.shape[0]))\n",
        "\n",
        "    feature_categories = categorize_features(train_features)\n",
        "\n",
        "    for i in range(num_iterations):\n",
        "        # Extract a subset of data\n",
        "        features_subset, labels_subset = extract_subset(train_features, train_labels, subset_size)\n",
        "\n",
        "        # Train a single random tree (using a subset of features)\n",
        "        single_tree = construct_tree(features_subset, labels_subset, feature_categories, max_features)\n",
        "        rf_single_tree_predictions[i] = tree_predict(test_features, single_tree)\n",
        "\n",
        "        # Train a random forest\n",
        "        forest_predictions_per_tree = np.zeros((num_trees, test_features.shape[0]))\n",
        "        for t in range(num_trees):\n",
        "            features_bag, labels_bag = extract_subset(train_features, train_labels, subset_size)\n",
        "            tree = construct_tree(features_bag, labels_bag, feature_categories, max_features)\n",
        "            forest_predictions_per_tree[t] = tree_predict(test_features, tree)\n",
        "\n",
        "        # Aggregate predictions for the forest\n",
        "        forest_predictions[i] = np.mean(forest_predictions_per_tree, axis=0)\n",
        "\n",
        "    # Compute bias and variance\n",
        "    rf_single_tree_bias, rf_single_tree_variance = compute_bias_variance(rf_single_tree_predictions, test_labels)\n",
        "    forest_bias, forest_variance = compute_bias_variance(forest_predictions, test_labels)\n",
        "\n",
        "    return rf_single_tree_bias, rf_single_tree_variance, forest_bias, forest_variance\n",
        "\n",
        "# Function to compute bias and variance\n",
        "def compute_bias_variance(predictions, true_labels):\n",
        "    bias_squared = np.mean((np.mean(predictions, axis=0) - true_labels) ** 2)\n",
        "    variance = np.mean(np.var(predictions, axis=0))\n",
        "    return bias_squared, variance\n",
        "\n",
        "def preprocess_dataset(dataframe):\n",
        "    categorical_cols = dataframe.select_dtypes(include=['object']).columns\n",
        "    for col in categorical_cols:\n",
        "        dataframe[col] = pd.Categorical(dataframe[col]).codes\n",
        "    return dataframe\n",
        "\n",
        "# Main execution function\n",
        "def execute_analysis():\n",
        "    # Load and preprocess the data\n",
        "    column_names = ['age', 'job', 'marital', 'education', 'default', 'balance',\n",
        "                    'housing', 'loan', 'contact', 'day', 'month', 'duration',\n",
        "                    'campaign', 'pdays', 'previous', 'poutcome', 'label']\n",
        "\n",
        "    train_dataset = pd.read_csv(\"train.csv\", names=column_names)\n",
        "    test_dataset = pd.read_csv(\"test.csv\", names=column_names)\n",
        "\n",
        "    # Preprocess the datasets\n",
        "    train_dataset = preprocess_dataset(train_dataset)\n",
        "    test_dataset = preprocess_dataset(test_dataset)\n",
        "\n",
        "    # Separate features and labels\n",
        "    train_features = train_dataset.drop('label', axis=1).values\n",
        "    train_labels = np.where(train_dataset['label'] == 1, 1, -1)\n",
        "    test_features = test_dataset.drop('label', axis=1).values\n",
        "    test_labels = np.where(test_dataset['label'] == 1, 1, -1)\n",
        "\n",
        "    # Perform ensemble learning analysis\n",
        "    single_tree_bias, single_tree_variance, bagged_tree_bias, bagged_tree_variance = ensemble_learning(train_features, train_labels, test_features, test_labels)\n",
        "\n",
        "    # Perform random forest analysis\n",
        "    rf_single_tree_bias, rf_single_tree_variance, forest_bias, forest_variance = random_forest_learning(train_features, train_labels, test_features, test_labels)\n",
        "\n",
        "    # Display results\n",
        "    print(f\"Single Tree - Bias^2: {single_tree_bias}, Variance: {single_tree_variance}, Total Error: {single_tree_bias + single_tree_variance}\")\n",
        "    print(f\"Bagged Trees - Bias^2: {bagged_tree_bias}, Variance: {bagged_tree_variance}, Total Error: {bagged_tree_bias + bagged_tree_variance}\")\n",
        "    print(f\"Single Random Tree - Bias^2: {rf_single_tree_bias}, Variance: {rf_single_tree_variance}, Total Error: {rf_single_tree_bias + rf_single_tree_variance}\")\n",
        "    print(f\"Random Forest - Bias^2: {forest_bias}, Variance: {forest_variance}, Total Error: {forest_bias + forest_variance}\")\n",
        "\n",
        "# Entry point of the script\n",
        "if __name__ == \"__main__\":\n",
        "    execute_analysis()"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "X2KNRkFt3vjh"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}