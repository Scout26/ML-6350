{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bHXod4y_rfAY",
        "outputId": "c74772cf-b43d-4f9b-9786-9c64de78f970"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Single Tree Bias^2: 0.34012800000000004, Variance: 0.3541688888888889, General Squared Error: 0.694296888888889\n",
            "Bagged Trees Bias^2: 0.29985185999999997, Variance: 0.017175044444444447, General Squared Error: 0.3170269044444444\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from collections import Counter\n",
        "\n",
        "# Function to randomly sample data without replacement\n",
        "def select_sample(X, y, sample_size=1000):\n",
        "    indices = np.random.choice(X.shape[0], sample_size, replace=False)\n",
        "    return X[indices], y[indices]\n",
        "\n",
        "# Function to initialize a decision tree node\n",
        "def initialize_node(value=None):\n",
        "    return {'value': value, 'left': None, 'right': None, 'feature': None, 'threshold': None}\n",
        "\n",
        "# Function to check if a node is a terminal/leaf node\n",
        "def is_terminal_node(node):\n",
        "    return node['value'] is not None\n",
        "\n",
        "# Function to identify feature types (categorical/numerical)\n",
        "def identify_feature_types(X):\n",
        "    feature_types = []\n",
        "    for i in range(X.shape[1]):\n",
        "        unique_vals = np.unique(X[:, i])\n",
        "        if isinstance(unique_vals[0], (int, float)) and len(unique_vals) > 10:\n",
        "            feature_types.append(\"numerical\")\n",
        "        else:\n",
        "            feature_types.append(\"categorical\")\n",
        "    return feature_types\n",
        "\n",
        "# Function to split the dataset based on a feature and a threshold\n",
        "def partition_data(X, feature, threshold):\n",
        "    left_indices = np.where(X[:, feature] <= threshold)[0]\n",
        "    right_indices = np.where(X[:, feature] > threshold)[0]\n",
        "    return left_indices, right_indices\n",
        "\n",
        "# Function to compute entropy for the target labels\n",
        "def calculate_entropy(y):\n",
        "    label_counts = Counter(y)\n",
        "    total_samples = len(y)\n",
        "    entropy_val = 0.0\n",
        "    for count in label_counts.values():\n",
        "        probability = count / total_samples\n",
        "        if probability > 0:\n",
        "            entropy_val -= probability * np.log2(probability)\n",
        "    return entropy_val\n",
        "\n",
        "# Function to compute information gain based on entropy\n",
        "def compute_information_gain(y, left_y, right_y):\n",
        "    total_samples = len(y)\n",
        "    parent_entropy = calculate_entropy(y)\n",
        "    left_entropy = calculate_entropy(left_y)\n",
        "    right_entropy = calculate_entropy(right_y)\n",
        "    weighted_entropy = (len(left_y) / total_samples) * left_entropy + (len(right_y) / total_samples) * right_entropy\n",
        "    return parent_entropy - weighted_entropy\n",
        "\n",
        "# Function to determine the best split for the data\n",
        "def find_optimal_split(X, y, feature_types):\n",
        "    best_gain = -np.inf\n",
        "    optimal_feature = None\n",
        "    optimal_threshold = None\n",
        "    num_features = X.shape[1]\n",
        "\n",
        "    for feature in range(num_features):\n",
        "        if feature_types[feature] == \"numerical\":\n",
        "            thresholds = np.unique(X[:, feature])\n",
        "            for threshold in thresholds:\n",
        "                left_indices, right_indices = partition_data(X, feature, threshold)\n",
        "                if len(left_indices) == 0 or len(right_indices) == 0:\n",
        "                    continue\n",
        "                gain = compute_information_gain(y, y[left_indices], y[right_indices])\n",
        "                if gain > best_gain:\n",
        "                    best_gain = gain\n",
        "                    optimal_feature = feature\n",
        "                    optimal_threshold = threshold\n",
        "        else:\n",
        "            unique_vals = np.unique(X[:, feature])\n",
        "            for value in unique_vals:\n",
        "                left_indices, right_indices = partition_data(X, feature, value)\n",
        "                if len(left_indices) == 0 or len(right_indices) == 0:\n",
        "                    continue\n",
        "                gain = compute_information_gain(y, y[left_indices], y[right_indices])\n",
        "                if gain > best_gain:\n",
        "                    best_gain = gain\n",
        "                    optimal_feature = feature\n",
        "                    optimal_threshold = value\n",
        "\n",
        "    return optimal_feature, optimal_threshold\n",
        "\n",
        "# Function to construct the decision tree\n",
        "def construct_tree(X, y, feature_types):\n",
        "    if len(np.unique(y)) == 1:\n",
        "        return initialize_node(value=y[0])\n",
        "\n",
        "    optimal_feature, optimal_threshold = find_optimal_split(X, y, feature_types)\n",
        "\n",
        "    if optimal_feature is None:\n",
        "        return initialize_node(value=Counter(y).most_common(1)[0][0])\n",
        "\n",
        "    left_indices, right_indices = partition_data(X, optimal_feature, optimal_threshold)\n",
        "\n",
        "    if len(left_indices) == 0 or len(right_indices) == 0:\n",
        "        return initialize_node(value=Counter(y).most_common(1)[0][0])\n",
        "\n",
        "    left_branch = construct_tree(X[left_indices], y[left_indices], feature_types)\n",
        "    right_branch = construct_tree(X[right_indices], y[right_indices], feature_types)\n",
        "\n",
        "    root_node = initialize_node()\n",
        "    root_node['feature'] = optimal_feature\n",
        "    root_node['threshold'] = optimal_threshold\n",
        "    root_node['left'] = left_branch\n",
        "    root_node['right'] = right_branch\n",
        "\n",
        "    return root_node\n",
        "\n",
        "# Function to make predictions using a decision tree\n",
        "def generate_predictions(X, tree):\n",
        "    predictions = np.zeros(X.shape[0])\n",
        "    for i, sample in enumerate(X):\n",
        "        current_node = tree\n",
        "        while not is_terminal_node(current_node):\n",
        "            if sample[current_node['feature']] <= current_node['threshold']:\n",
        "                current_node = current_node['left']\n",
        "            else:\n",
        "                current_node = current_node['right']\n",
        "        predictions[i] = current_node['value']\n",
        "    return predictions\n",
        "\n",
        "# Main function implementing bagging and bias-variance decomposition\n",
        "def bagging_algorithm(X_train, y_train, X_test, y_test, iterations=10, num_trees=20, sample_size=300):\n",
        "    single_tree_predictions = np.zeros((iterations, X_test.shape[0]))\n",
        "    aggregated_bagging_predictions = np.zeros((iterations, X_test.shape[0]))\n",
        "\n",
        "    feature_types = identify_feature_types(X_train)\n",
        "\n",
        "    for i in range(iterations):\n",
        "        # Select sample without replacement\n",
        "        X_sample, y_sample = select_sample(X_train, y_train, sample_size)\n",
        "\n",
        "        # Train a single decision tree\n",
        "        single_tree_model = construct_tree(X_sample, y_sample, feature_types)\n",
        "        single_tree_predictions[i] = generate_predictions(X_test, single_tree_model)\n",
        "\n",
        "        # Train an ensemble of decision trees using bagging\n",
        "        ensemble_predictions = np.zeros((num_trees, X_test.shape[0]))\n",
        "        for t in range(num_trees):\n",
        "            X_bagged_sample, y_bagged_sample = select_sample(X_train, y_train, sample_size)\n",
        "            tree = construct_tree(X_bagged_sample, y_bagged_sample, feature_types)\n",
        "            ensemble_predictions[t] = generate_predictions(X_test, tree)\n",
        "\n",
        "        # Average predictions for bagged trees\n",
        "        aggregated_bagging_predictions[i] = np.mean(ensemble_predictions, axis=0)\n",
        "\n",
        "    # Compute bias and variance for single decision trees\n",
        "    single_tree_bias, single_tree_variance = calculate_bias_variance(single_tree_predictions, y_test)\n",
        "\n",
        "    # Compute bias and variance for bagged decision trees\n",
        "    bagged_bias, bagged_variance = calculate_bias_variance(aggregated_bagging_predictions, y_test)\n",
        "\n",
        "    return single_tree_bias, single_tree_variance, bagged_bias, bagged_variance\n",
        "\n",
        "# Function to calculate bias and variance\n",
        "def calculate_bias_variance(predicted_outputs, true_labels):\n",
        "    average_prediction = np.mean(predicted_outputs, axis=0)\n",
        "    bias_squared = np.mean((average_prediction - true_labels) ** 2)\n",
        "    variance = np.mean(np.var(predicted_outputs, axis=0, ddof=1))\n",
        "    return bias_squared, variance\n",
        "\n",
        "# Data preprocessing: Convert categorical features to numeric\n",
        "def transform_data(df):\n",
        "    categorical_cols = df.select_dtypes(include=['object']).columns\n",
        "    for column in categorical_cols:\n",
        "        df[column] = pd.Categorical(df[column]).codes\n",
        "    return df\n",
        "\n",
        "# Main function for executing the workflow\n",
        "def run():\n",
        "    # Load the training and test datasets\n",
        "    headers = ['age', 'job', 'marital', 'education', 'default', 'balance',\n",
        "               'housing', 'loan', 'contact', 'day', 'month', 'duration',\n",
        "               'campaign', 'pdays', 'previous', 'poutcome', 'label']\n",
        "\n",
        "    train_df = pd.read_csv(\"train.csv\", names=headers)\n",
        "    test_df = pd.read_csv(\"test.csv\", names=headers)\n",
        "\n",
        "    # Transform the datasets by converting categorical to numeric features\n",
        "    train_df = transform_data(train_df)\n",
        "    test_df = transform_data(test_df)\n",
        "\n",
        "    # Separate features and labels for training and testing\n",
        "    X_train = train_df.drop('label', axis=1).values\n",
        "    y_train = np.where(train_df['label'] == 1, 1, -1)\n",
        "    X_test = test_df.drop('label', axis=1).values\n",
        "    y_test = np.where(test_df['label'] == 1, 1, -1)\n",
        "\n",
        "    # Perform bagging and bias-variance analysis\n",
        "    single_tree_bias, single_tree_variance, bagged_bias, bagged_variance = bagging_algorithm(X_train, y_train, X_test, y_test)\n",
        "\n",
        "    # Display the results\n",
        "    print(f\"Single Tree Bias^2: {single_tree_bias}, Variance: {single_tree_variance}, General Squared Error: {single_tree_bias + single_tree_variance}\")\n",
        "    print(f\"Bagged Trees Bias^2: {bagged_bias}, Variance: {bagged_variance}, General Squared Error: {bagged_bias + bagged_variance}\")\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    run()\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "raSh12suu25B"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}