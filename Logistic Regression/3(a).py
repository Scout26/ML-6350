# -*- coding: utf-8 -*-
"""3(a)

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1NQLOWoQwgp907cDFDBrQ4xkFKlqMuxAQ
"""

import numpy as np
import pandas as pd

# Load dataset from files, split into features and labels
# Adjust labels to be in the range {-1, 1} for consistency

def load_data(train_file, test_file):
    train_data = np.loadtxt(train_file, delimiter=',')
    test_data = np.loadtxt(test_file, delimiter=',')
    X_train, y_train = train_data[:, :-1], train_data[:, -1]
    X_test, y_test = test_data[:, :-1], test_data[:, -1]
    y_train = 2 * y_train - 1  # Transform labels to {-1, 1}
    y_test = 2 * y_test - 1  # Transform labels to {-1, 1}
    return X_train, y_train, X_test, y_test

# Compute sigmoid function using a numerically stable approach
# Handles large positive and negative values of z to avoid overflow

def sigmoid(z):
    return np.where(z >= 0, 1 / (1 + np.exp(-z)), np.exp(z) / (1 + np.exp(z)))

# Evaluate the objective function, incorporating logistic loss and L2 regularization
# Gaussian prior contributes to the regularization term

def compute_loss(w, X, y, v):
    m = X.shape[0]  # Number of training examples
    predictions = np.clip(sigmoid(X @ w), 1e-8, 1 - 1e-8)  # Avoid log(0) issues
    log_loss = -np.sum(y * np.log(predictions) + (1 - y) * np.log(1 - predictions)) / m
    regularization = np.sum(w**2) / (2 * (v + 1e-8))  # L2 regularization term
    return log_loss + regularization

# Calculate gradients of the objective function with respect to the weights
# Includes gradients for logistic loss and regularization

def compute_gradient(w, X, y, v):
    m = X.shape[0]
    predictions = sigmoid(X @ w)
    loss_gradient = (X.T @ (predictions - y)) / m  # Gradient from logistic loss
    regularization_grad = w / (v + 1e-8)  # Gradient from regularization
    return loss_gradient + regularization_grad

# Train logistic regression using stochastic gradient descent
# Incorporates L2 regularization using a prior variance v

def logistic_regression_with_map(X_train, y_train, X_test, y_test, v_values, gamma_0, d, T):
    n_features = X_train.shape[1]
    results = {}

    for v in v_values:
        w = np.zeros(n_features)  # Initialize weight vector
        epoch_losses = []  # Track loss values per epoch

        for epoch in range(T):
            indices = np.random.permutation(X_train.shape[0])  # Shuffle training data
            X_train, y_train = X_train[indices], y_train[indices]

            for i in range(X_train.shape[0]):
                gamma_t = gamma_0 / (1 + (gamma_0 / d) * epoch)  # Decaying learning rate
                gradient = compute_gradient(w, X_train[i:i+1], y_train[i:i+1], v)
                w -= gamma_t * gradient  # Update weights
                # print(w)
            # Calculate loss for the epoch
            loss = compute_loss(w, X_train, y_train, v)
            epoch_losses.append(loss)

        # Evaluate model performance on test data
        test_predictions = sigmoid(X_test @ w) >= 0.5
        test_accuracy = np.mean(test_predictions == (y_test > 0))

        results[v] = {
            "weights": w,
            "losses": epoch_losses,
            "test_accuracy": test_accuracy
        }
        #print(f"Variance (v): {v}, Test Accuracy: {test_accuracy:.4f}")

    return results

# Function to print results in a table
def print_results_table(results):
    rows = []
    for v, data in results.items():
        rows.append({
            "Variance (v)": v,
            "Test Accuracy": f"{data['test_accuracy']:.4f}",
        })

    # Create a DataFrame and display it
    results_table = pd.DataFrame(rows)
    print(results_table)

# Main script to execute logistic regression
train_file = 'train.csv'  # File containing training data
test_file = 'test.csv'  # File containing test data
X_train, y_train, X_test, y_test = load_data(train_file, test_file)

# Hyperparameter settings
v_values = [0.01, 0.1, 0.5, 1, 3, 5, 10, 100]  # Variance values for regularization
gamma_0 = 0.1  # Initial learning rate
d = 1  # Decay parameter for learning rate
T = 100  # Number of training epochs

# Train model and evaluate results
results = logistic_regression_with_map(X_train, y_train, X_test, y_test, v_values, gamma_0, d, T)

# Print the results as a table
print_results_table(results)

